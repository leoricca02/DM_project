{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f4a9914",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN ETL PIPELINE - ARCHITETTURA 3 LIVELLI\n",
    "# Orchestrator principale che coordina tutto il processo ETL\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "import subprocess\n",
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b7b3c1f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 CONFIGURAZIONE SISTEMA\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURAZIONE\n",
    "# =============================================================================\n",
    "print(\"🔧 CONFIGURAZIONE SISTEMA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Connessione database\n",
    "engine = create_engine('postgresql://postgres:postgres@localhost:5432/ecommerce')\n",
    "\n",
    "# Path dei file\n",
    "BASE_PATH = '1_source_layer/datasets_olist/'\n",
    "RDL_SCHEMA_PATH = '2_rdl_layer/create_rdl_schema.sql'\n",
    "DW_SCHEMA_PATH = '3_dw_layer/create_tables.sql'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "059275df-2ebb-4e8a-8ee2-7f69c87addc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 FASE 0: SETUP DATABASE\n",
      "------------------------------------------------------------\n",
      "📦 Creazione schema RDL...\n",
      "✅ Schema RDL creato\n",
      "📦 Creazione schema DW...\n",
      "✅ Schema DW creato\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FASE 0: SETUP DATABASE\n",
    "# =============================================================================\n",
    "print(\"\\n📋 FASE 0: SETUP DATABASE\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "def setup_database_schemas():\n",
    "    \"\"\"Crea gli schemi RDL e DW\"\"\"\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            # Crea schema RDL\n",
    "            print(\"📦 Creazione schema RDL...\")\n",
    "            with open(RDL_SCHEMA_PATH, 'r') as f:\n",
    "                rdl_sql = f.read()\n",
    "            conn.execute(text(rdl_sql))\n",
    "            conn.commit()\n",
    "            print(\"✅ Schema RDL creato\")\n",
    "            \n",
    "            # Crea schema DW (le tue tabelle esistenti)\n",
    "            print(\"📦 Creazione schema DW...\")\n",
    "            with open(DW_SCHEMA_PATH, 'r') as f:\n",
    "                dw_sql = f.read()\n",
    "            conn.execute(text(dw_sql))\n",
    "            conn.commit()\n",
    "            print(\"✅ Schema DW creato\")\n",
    "            \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Errore setup database: {e}\")\n",
    "        return False\n",
    "\n",
    "# Esegui setup\n",
    "setup_database_schemas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17d97172-7d15-4b2f-969a-15f656c35f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📥 FASE 1: CARICAMENTO DATI SORGENTE\n",
      "------------------------------------------------------------\n",
      "📊 Caricando olist_orders_dataset.csv...\n",
      "✅ orders: 99,441 righe\n",
      "📊 Caricando olist_order_items_dataset.csv...\n",
      "✅ order_items: 112,650 righe\n",
      "📊 Caricando olist_customers_dataset.csv...\n",
      "✅ customers: 99,441 righe\n",
      "📊 Caricando olist_products_dataset.csv...\n",
      "✅ products: 32,951 righe\n",
      "📊 Caricando olist_sellers_dataset.csv...\n",
      "✅ sellers: 3,095 righe\n",
      "📊 Caricando olist_order_payments_dataset.csv...\n",
      "✅ payments: 103,886 righe\n",
      "📊 Caricando olist_order_reviews_dataset.csv...\n",
      "✅ reviews: 99,224 righe\n",
      "📊 Caricando olist_geolocation_dataset.csv...\n",
      "✅ geolocation: 1,000,163 righe\n",
      "📊 Caricando product_category_name_translation.csv...\n",
      "✅ translation: 71 righe\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FASE 1: CARICAMENTO DATI SORGENTE\n",
    "# =============================================================================\n",
    "print(\"\\n📥 FASE 1: CARICAMENTO DATI SORGENTE\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Il tuo codice esistente per caricare i CSV\n",
    "files_to_load = {\n",
    "    'orders': 'olist_orders_dataset.csv',\n",
    "    'order_items': 'olist_order_items_dataset.csv', \n",
    "    'customers': 'olist_customers_dataset.csv',\n",
    "    'products': 'olist_products_dataset.csv',\n",
    "    'sellers': 'olist_sellers_dataset.csv',\n",
    "    'payments': 'olist_order_payments_dataset.csv',\n",
    "    'reviews': 'olist_order_reviews_dataset.csv',\n",
    "    'geolocation': 'olist_geolocation_dataset.csv',\n",
    "    'translation': 'product_category_name_translation.csv'\n",
    "}\n",
    "\n",
    "source_data = {}\n",
    "for name, filename in files_to_load.items():\n",
    "    filepath = BASE_PATH + filename\n",
    "    print(f\"📊 Caricando {filename}...\")\n",
    "    source_data[name] = pd.read_csv(filepath)\n",
    "    print(f\"✅ {name}: {len(source_data[name]):,} righe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d779215f-37bb-4166-88c2-b1e2c0a01233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 VERIFICA STRUTTURA TABELLE RDL:\n",
      "------------------------------------------------------------\n",
      "\n",
      "📋 Tabella rdl.orders:\n",
      "   - order_id (character varying)\n",
      "   - customer_id (character varying)\n",
      "   - order_status (character varying)\n",
      "   - order_purchase_timestamp (timestamp without time zone)\n",
      "   - order_approved_at (timestamp without time zone)\n",
      "   - order_delivered_carrier_date (timestamp without time zone)\n",
      "   - order_delivered_customer_date (timestamp without time zone)\n",
      "   - order_estimated_delivery_date (timestamp without time zone)\n",
      "   - rdl_load_timestamp (timestamp without time zone)\n",
      "   - rdl_source_system (character varying)\n",
      "   - rdl_is_valid (boolean)\n",
      "   - rdl_validation_errors (text)\n",
      "   - rdl_processing_status (character varying)\n",
      "   - rdl_delivery_delay_days (integer)\n",
      "   - rdl_approval_delay_hours (integer)\n",
      "\n",
      "📋 Tabella rdl.customers:\n",
      "   - customer_id (character varying)\n",
      "   - customer_unique_id (character varying)\n",
      "   - customer_zip_code_prefix (integer)\n",
      "   - customer_city (character varying)\n",
      "   - customer_state (character varying)\n",
      "   - rdl_load_timestamp (timestamp without time zone)\n",
      "   - rdl_source_system (character varying)\n",
      "   - rdl_is_valid (boolean)\n",
      "   - rdl_validation_errors (text)\n",
      "   - rdl_latitude (numeric)\n",
      "   - rdl_longitude (numeric)\n",
      "   - rdl_geocoding_status (character varying)\n",
      "   - rdl_geocoding_confidence (numeric)\n",
      "   - rdl_address_completeness_score (numeric)\n",
      "   - rdl_data_quality_score (numeric)\n",
      "\n",
      "📋 Tabella rdl.products:\n",
      "   - product_id (character varying)\n",
      "   - product_category_name (character varying)\n",
      "   - product_name_lenght (integer)\n",
      "   - product_description_lenght (integer)\n",
      "   - product_photos_qty (integer)\n",
      "   - product_weight_g (integer)\n",
      "   - product_length_cm (integer)\n",
      "   - product_height_cm (integer)\n",
      "   - product_width_cm (integer)\n",
      "   - rdl_load_timestamp (timestamp without time zone)\n",
      "   - rdl_source_system (character varying)\n",
      "   - rdl_is_valid (boolean)\n",
      "   - rdl_validation_errors (text)\n",
      "   - rdl_category_english (character varying)\n",
      "   - rdl_category_hierarchy_1 (character varying)\n",
      "   - rdl_category_hierarchy_2 (character varying)\n",
      "   - rdl_weight_category (character varying)\n",
      "   - rdl_size_category (character varying)\n",
      "   - rdl_volume_cm3 (integer)\n",
      "   - rdl_has_complete_dimensions (boolean)\n",
      "   - rdl_has_photos (boolean)\n",
      "   - rdl_description_quality (character varying)\n",
      "\n",
      "📋 Tabella rdl.order_items:\n",
      "   - order_id (character varying)\n",
      "   - order_item_id (integer)\n",
      "   - product_id (character varying)\n",
      "   - seller_id (character varying)\n",
      "   - shipping_limit_date (timestamp without time zone)\n",
      "   - price (numeric)\n",
      "   - freight_value (numeric)\n",
      "   - rdl_load_timestamp (timestamp without time zone)\n",
      "   - rdl_source_system (character varying)\n",
      "   - rdl_is_valid (boolean)\n",
      "   - rdl_total_item_value (numeric)\n",
      "   - rdl_freight_percentage (numeric)\n",
      "   - rdl_item_profit_estimate (numeric)\n",
      "\n",
      "📋 Tabella rdl.payments:\n",
      "   - order_id (character varying)\n",
      "   - payment_sequential (integer)\n",
      "   - payment_type (character varying)\n",
      "   - payment_installments (integer)\n",
      "   - payment_value (numeric)\n",
      "   - rdl_load_timestamp (timestamp without time zone)\n",
      "   - rdl_source_system (character varying)\n",
      "   - rdl_is_valid (boolean)\n",
      "   - rdl_total_order_value (numeric)\n",
      "   - rdl_payment_method_count (integer)\n",
      "   - rdl_is_multi_payment (boolean)\n",
      "\n",
      "📋 Tabella rdl.reviews:\n",
      "   - review_id (character varying)\n",
      "   - order_id (character varying)\n",
      "   - review_score (integer)\n",
      "   - review_comment_title (character varying)\n",
      "   - review_comment_message (text)\n",
      "   - review_creation_date (timestamp without time zone)\n",
      "   - review_answer_timestamp (timestamp without time zone)\n",
      "   - rdl_load_timestamp (timestamp without time zone)\n",
      "   - rdl_source_system (character varying)\n",
      "   - rdl_is_valid (boolean)\n",
      "   - rdl_has_comment (boolean)\n",
      "   - rdl_comment_length (integer)\n",
      "   - rdl_response_time_hours (integer)\n",
      "   - rdl_sentiment_score (numeric)\n",
      "   - rdl_review_category (character varying)\n",
      "\n",
      "📋 Tabella rdl.sellers:\n",
      "   - seller_id (character varying)\n",
      "   - seller_zip_code_prefix (integer)\n",
      "   - seller_city (character varying)\n",
      "   - seller_state (character varying)\n",
      "   - rdl_load_timestamp (timestamp without time zone)\n",
      "   - rdl_source_system (character varying)\n",
      "   - rdl_is_valid (boolean)\n",
      "   - rdl_latitude (numeric)\n",
      "   - rdl_longitude (numeric)\n",
      "   - rdl_geocoding_status (character varying)\n",
      "   - rdl_geocoding_confidence (numeric)\n",
      "   - rdl_total_orders (integer)\n",
      "   - rdl_total_revenue (numeric)\n",
      "   - rdl_avg_rating (numeric)\n",
      "   - rdl_active_status (character varying)\n",
      "\n",
      "📋 Tabella rdl.geolocation:\n",
      "   - geolocation_zip_code_prefix (integer)\n",
      "   - geolocation_lat (numeric)\n",
      "   - geolocation_lng (numeric)\n",
      "   - geolocation_city (character varying)\n",
      "   - geolocation_state (character varying)\n",
      "   - rdl_load_timestamp (timestamp without time zone)\n",
      "   - rdl_is_primary (boolean)\n",
      "   - rdl_confidence_score (numeric)\n",
      "   - rdl_region (character varying)\n"
     ]
    }
   ],
   "source": [
    "# VERIFICA STRUTTURA TABELLE RDL\n",
    "print(\"🔍 VERIFICA STRUTTURA TABELLE RDL:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    tables = ['orders', 'customers', 'products', 'order_items', 'payments', 'reviews', 'sellers', 'geolocation']\n",
    "    \n",
    "    for table in tables:\n",
    "        print(f\"\\n📋 Tabella rdl.{table}:\")\n",
    "        try:\n",
    "            # Query per ottenere le colonne della tabella\n",
    "            result = conn.execute(text(f\"\"\"\n",
    "                SELECT column_name, data_type \n",
    "                FROM information_schema.columns \n",
    "                WHERE table_schema = 'rdl' \n",
    "                AND table_name = '{table}'\n",
    "                ORDER BY ordinal_position\n",
    "            \"\"\"))\n",
    "            \n",
    "            columns = result.fetchall()\n",
    "            if columns:\n",
    "                for col_name, data_type in columns:\n",
    "                    print(f\"   - {col_name} ({data_type})\")\n",
    "            else:\n",
    "                print(f\"   ❌ Tabella non trovata o senza colonne\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Errore: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67423f2d-a591-4f3c-b30b-1c44dbe086f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 FASE 2: ETL VERSO RDL (Versione definitiva)\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔍 ANALISI PROBLEMA GEOLOCATION:\n",
      "Record totali: 1,000,163\n",
      "Duplicati trovati: 411,724\n",
      "\n",
      "Esempi di duplicati:\n",
      "  ZIP: 1037, LAT: -23.54562128, LNG: -46.63929205\n",
      "  ZIP: 1046, LAT: -23.54608113, LNG: -46.6448203\n",
      "  ZIP: 1046, LAT: -23.54612897, LNG: -46.64295148\n",
      "  ZIP: 1047, LAT: -23.54627311, LNG: -46.64122517\n",
      "  ZIP: 1013, LAT: -23.54692321, LNG: -46.6342637\n",
      "  ZIP: 1029, LAT: -23.54376906, LNG: -46.63427784\n",
      "  ZIP: 1011, LAT: -23.54763955, LNG: -46.63603162\n",
      "  ZIP: 1013, LAT: -23.54732513, LNG: -46.63418379\n",
      "  ZIP: 1012, LAT: -23.54894599, LNG: -46.63467113\n",
      "  ZIP: 1037, LAT: -23.54518734, LNG: -46.63785524\n",
      "\n",
      "📍 Caricamento speciale GEOLOCATION...\n",
      "   Record originali: 1,000,163\n",
      "   Record dopo deduplicazione: 720,148\n",
      "   Caricamento in batch...\n",
      "✅ GEOLOCATION caricata: 720,148 record\n",
      "\n",
      "📥 Caricamento altre tabelle RDL...\n",
      "✅ orders: 99,441 record caricati\n",
      "✅ customers: 99,441 record caricati\n",
      "✅ products: 32,951 record caricati\n",
      "✅ order_items: 112,650 record caricati\n",
      "✅ payments: 103,886 record caricati\n",
      "   ⚠️ Rimossi 814 reviews duplicati\n",
      "✅ reviews: 98,410 record caricati\n",
      "✅ sellers: 3,095 record caricati\n",
      "\n",
      "📊 REPORT FINALE RDL:\n",
      "------------------------------------------------------------\n",
      "rdl.orders: 99,441 record\n",
      "rdl.customers: 99,441 record\n",
      "rdl.products: 32,951 record\n",
      "rdl.order_items: 112,650 record\n",
      "rdl.payments: 103,886 record\n",
      "rdl.reviews: 98,410 record\n",
      "rdl.sellers: 3,095 record\n",
      "rdl.geolocation: 720,148 record\n",
      "\n",
      "✅ FASE 2 completata con successo!\n"
     ]
    }
   ],
   "source": [
    "# FASE 2 - VERSIONE DEFINITIVA CON DEBUG E FIX COMPLETO\n",
    "print(\"\\n🔄 FASE 2: ETL VERSO RDL (Versione definitiva)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Prima di tutto, analizziamo il problema geolocation\n",
    "print(\"\\n🔍 ANALISI PROBLEMA GEOLOCATION:\")\n",
    "geo_df = source_data['geolocation'].copy()\n",
    "\n",
    "# Converti i tipi per assicurare consistenza\n",
    "geo_df['geolocation_zip_code_prefix'] = geo_df['geolocation_zip_code_prefix'].astype(int)\n",
    "geo_df['geolocation_lat'] = geo_df['geolocation_lat'].round(8)  # Precision come nel DB\n",
    "geo_df['geolocation_lng'] = geo_df['geolocation_lng'].round(8)\n",
    "\n",
    "print(f\"Record totali: {len(geo_df):,}\")\n",
    "\n",
    "# Trova duplicati ESATTI\n",
    "duplicates = geo_df[geo_df.duplicated(\n",
    "    subset=['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng'], \n",
    "    keep=False\n",
    ")]\n",
    "print(f\"Duplicati trovati: {len(duplicates):,}\")\n",
    "\n",
    "# Mostra alcuni esempi di duplicati\n",
    "if len(duplicates) > 0:\n",
    "    print(\"\\nEsempi di duplicati:\")\n",
    "    sample = duplicates.head(10)\n",
    "    for _, row in sample.iterrows():\n",
    "        print(f\"  ZIP: {row['geolocation_zip_code_prefix']}, \"\n",
    "              f\"LAT: {row['geolocation_lat']}, LNG: {row['geolocation_lng']}\")\n",
    "\n",
    "# Funzione per pulire COMPLETAMENTE il database\n",
    "def clean_rdl_completely():\n",
    "    \"\"\"Pulisce completamente lo schema RDL\"\"\"\n",
    "    print(\"\\n🧹 PULIZIA COMPLETA RDL...\")\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            # Prima elimina le viste che dipendono dalle tabelle\n",
    "            conn.execute(text(\"DROP VIEW IF EXISTS rdl.v_data_quality_summary CASCADE\"))\n",
    "            conn.execute(text(\"DROP VIEW IF EXISTS rdl.v_geocoding_status CASCADE\"))\n",
    "            conn.commit()\n",
    "            \n",
    "            # Poi elimina tutte le tabelle\n",
    "            tables = ['fact_sales', 'orders', 'customers', 'products', 'order_items', \n",
    "                     'payments', 'reviews', 'sellers', 'geolocation', \n",
    "                     'data_quality_log', 'etl_process_log', 'data_lineage']\n",
    "            \n",
    "            for table in tables:\n",
    "                try:\n",
    "                    conn.execute(text(f\"DROP TABLE IF EXISTS rdl.{table} CASCADE\"))\n",
    "                    print(f\"   ✓ Eliminata rdl.{table}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   ⚠️ rdl.{table}: {e}\")\n",
    "            \n",
    "            conn.commit()\n",
    "            print(\"✅ Schema RDL pulito completamente\")\n",
    "            \n",
    "            # Ricrea solo le tabelle necessarie\n",
    "            print(\"\\n📦 Ricreazione tabelle RDL essenziali...\")\n",
    "            \n",
    "            # Ricrea geolocation con gestione migliore\n",
    "            conn.execute(text(\"\"\"\n",
    "                CREATE TABLE rdl.geolocation (\n",
    "                    geolocation_zip_code_prefix INTEGER,\n",
    "                    geolocation_lat NUMERIC(10,8),\n",
    "                    geolocation_lng NUMERIC(11,8),\n",
    "                    geolocation_city VARCHAR(100),\n",
    "                    geolocation_state VARCHAR(2),\n",
    "                    rdl_load_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    rdl_is_primary BOOLEAN DEFAULT TRUE,\n",
    "                    rdl_confidence_score NUMERIC(3,2) DEFAULT 1.0,\n",
    "                    rdl_region VARCHAR(50),\n",
    "                    CONSTRAINT geolocation_pkey PRIMARY KEY \n",
    "                        (geolocation_zip_code_prefix, geolocation_lat, geolocation_lng)\n",
    "                )\n",
    "            \"\"\"))\n",
    "            \n",
    "            # Ricrea le altre tabelle dal tuo schema originale\n",
    "            # -- =====================================================\n",
    "            conn.execute(text(\"\"\"\n",
    "                -- RECONCILED DATA LAYER (RDL) SCHEMA\n",
    "                -- Intermediate layer for data cleaning and integration\n",
    "                -- =====================================================\n",
    "                \n",
    "                -- Create separate schema for RDL\n",
    "                CREATE SCHEMA IF NOT EXISTS rdl;\n",
    "                \n",
    "                -- =====================================================\n",
    "                -- RDL TABLES - Mirror operational sources with additions\n",
    "                -- =====================================================\n",
    "                \n",
    "                -- RDL Orders (with data quality flags)\n",
    "                DROP TABLE IF EXISTS rdl.orders CASCADE;\n",
    "                CREATE TABLE rdl.orders (\n",
    "                    -- Original columns\n",
    "                    order_id VARCHAR(50) PRIMARY KEY,\n",
    "                    customer_id VARCHAR(50),\n",
    "                    order_status VARCHAR(50),\n",
    "                    order_purchase_timestamp TIMESTAMP,\n",
    "                    order_approved_at TIMESTAMP,\n",
    "                    order_delivered_carrier_date TIMESTAMP,\n",
    "                    order_delivered_customer_date TIMESTAMP,\n",
    "                    order_estimated_delivery_date TIMESTAMP,\n",
    "                    \n",
    "                    -- RDL additions for data quality\n",
    "                    rdl_load_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    rdl_source_system VARCHAR(50) DEFAULT 'olist',\n",
    "                    rdl_is_valid BOOLEAN DEFAULT TRUE,\n",
    "                    rdl_validation_errors TEXT,\n",
    "                    rdl_processing_status VARCHAR(20) DEFAULT 'new', -- new, processed, error\n",
    "                    \n",
    "                    -- Derived fields for reconciliation\n",
    "                    rdl_delivery_delay_days INTEGER,\n",
    "                    rdl_approval_delay_hours INTEGER\n",
    "                );\n",
    "                \n",
    "                -- RDL Customers (with geocoding status)\n",
    "                DROP TABLE IF EXISTS rdl.customers CASCADE;\n",
    "                CREATE TABLE rdl.customers (\n",
    "                    -- Original columns\n",
    "                    customer_id VARCHAR(50) PRIMARY KEY,\n",
    "                    customer_unique_id VARCHAR(50),\n",
    "                    customer_zip_code_prefix INTEGER,\n",
    "                    customer_city VARCHAR(100),\n",
    "                    customer_state VARCHAR(2),\n",
    "                    \n",
    "                    -- RDL additions\n",
    "                    rdl_load_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    rdl_source_system VARCHAR(50) DEFAULT 'olist',\n",
    "                    rdl_is_valid BOOLEAN DEFAULT TRUE,\n",
    "                    rdl_validation_errors TEXT,\n",
    "                    \n",
    "                    -- Geocoding fields\n",
    "                    rdl_latitude DECIMAL(10,8),\n",
    "                    rdl_longitude DECIMAL(11,8),\n",
    "                    rdl_geocoding_status VARCHAR(20), -- matched, approximate, not_found\n",
    "                    rdl_geocoding_confidence DECIMAL(3,2),\n",
    "                    \n",
    "                    -- Data quality scores\n",
    "                    rdl_address_completeness_score DECIMAL(3,2),\n",
    "                    rdl_data_quality_score DECIMAL(3,2)\n",
    "                );\n",
    "                \n",
    "                -- RDL Products (with enrichment fields)\n",
    "                DROP TABLE IF EXISTS rdl.products CASCADE;\n",
    "                CREATE TABLE rdl.products (\n",
    "                    -- Original columns\n",
    "                    product_id VARCHAR(50) PRIMARY KEY,\n",
    "                    product_category_name VARCHAR(100),\n",
    "                    product_name_lenght INTEGER,\n",
    "                    product_description_lenght INTEGER,\n",
    "                    product_photos_qty INTEGER,\n",
    "                    product_weight_g INTEGER,\n",
    "                    product_length_cm INTEGER,\n",
    "                    product_height_cm INTEGER,\n",
    "                    product_width_cm INTEGER,\n",
    "                    \n",
    "                    -- RDL additions\n",
    "                    rdl_load_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    rdl_source_system VARCHAR(50) DEFAULT 'olist',\n",
    "                    rdl_is_valid BOOLEAN DEFAULT TRUE,\n",
    "                    rdl_validation_errors TEXT,\n",
    "                    \n",
    "                    -- Enrichment fields\n",
    "                    rdl_category_english VARCHAR(100),\n",
    "                    rdl_category_hierarchy_1 VARCHAR(50), -- Main category\n",
    "                    rdl_category_hierarchy_2 VARCHAR(50), -- Subcategory\n",
    "                    rdl_weight_category VARCHAR(20),\n",
    "                    rdl_size_category VARCHAR(20),\n",
    "                    rdl_volume_cm3 INTEGER,\n",
    "                    \n",
    "                    -- Quality indicators\n",
    "                    rdl_has_complete_dimensions BOOLEAN,\n",
    "                    rdl_has_photos BOOLEAN,\n",
    "                    rdl_description_quality VARCHAR(20) -- short, medium, long\n",
    "                );\n",
    "                \n",
    "                -- RDL Order Items (with calculations)\n",
    "                DROP TABLE IF EXISTS rdl.order_items CASCADE;\n",
    "                CREATE TABLE rdl.order_items (\n",
    "                    -- Composite primary key\n",
    "                    order_id VARCHAR(50),\n",
    "                    order_item_id INTEGER,\n",
    "                    product_id VARCHAR(50),\n",
    "                    seller_id VARCHAR(50),\n",
    "                    shipping_limit_date TIMESTAMP,\n",
    "                    price DECIMAL(10,2),\n",
    "                    freight_value DECIMAL(10,2),\n",
    "                    \n",
    "                    -- RDL additions\n",
    "                    rdl_load_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    rdl_source_system VARCHAR(50) DEFAULT 'olist',\n",
    "                    rdl_is_valid BOOLEAN DEFAULT TRUE,\n",
    "                    \n",
    "                    -- Calculated fields\n",
    "                    rdl_total_item_value DECIMAL(10,2),\n",
    "                    rdl_freight_percentage DECIMAL(8,2),\n",
    "                    rdl_item_profit_estimate DECIMAL(10,2),\n",
    "                    \n",
    "                    PRIMARY KEY (order_id, order_item_id)\n",
    "                );\n",
    "                \n",
    "                -- RDL Payments (with aggregations)\n",
    "                DROP TABLE IF EXISTS rdl.payments CASCADE;\n",
    "                CREATE TABLE rdl.payments (\n",
    "                    -- Original columns\n",
    "                    order_id VARCHAR(50),\n",
    "                    payment_sequential INTEGER,\n",
    "                    payment_type VARCHAR(50),\n",
    "                    payment_installments INTEGER,\n",
    "                    payment_value DECIMAL(10,2),\n",
    "                    \n",
    "                    -- RDL additions\n",
    "                    rdl_load_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    rdl_source_system VARCHAR(50) DEFAULT 'olist',\n",
    "                    rdl_is_valid BOOLEAN DEFAULT TRUE,\n",
    "                    \n",
    "                    -- Aggregated at order level\n",
    "                    rdl_total_order_value DECIMAL(10,2),\n",
    "                    rdl_payment_method_count INTEGER,\n",
    "                    rdl_is_multi_payment BOOLEAN,\n",
    "                    \n",
    "                    PRIMARY KEY (order_id, payment_sequential)\n",
    "                );\n",
    "                \n",
    "                -- RDL Reviews (with sentiment analysis placeholder)\n",
    "                DROP TABLE IF EXISTS rdl.reviews CASCADE;\n",
    "                CREATE TABLE rdl.reviews (\n",
    "                    review_id VARCHAR(50) PRIMARY KEY,\n",
    "                    order_id VARCHAR(50),\n",
    "                    review_score INTEGER,\n",
    "                    review_comment_title VARCHAR(100),\n",
    "                    review_comment_message TEXT,\n",
    "                    review_creation_date TIMESTAMP,\n",
    "                    review_answer_timestamp TIMESTAMP,\n",
    "                    \n",
    "                    -- RDL additions\n",
    "                    rdl_load_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    rdl_source_system VARCHAR(50) DEFAULT 'olist',\n",
    "                    rdl_is_valid BOOLEAN DEFAULT TRUE,\n",
    "                    \n",
    "                    -- Text analysis placeholders\n",
    "                    rdl_has_comment BOOLEAN,\n",
    "                    rdl_comment_length INTEGER,\n",
    "                    rdl_response_time_hours INTEGER,\n",
    "                    rdl_sentiment_score DECIMAL(3,2), -- -1 to 1\n",
    "                    rdl_review_category VARCHAR(50) -- delivery, quality, price, etc.\n",
    "                );\n",
    "                \n",
    "                -- RDL Sellers (with business metrics)\n",
    "                DROP TABLE IF EXISTS rdl.sellers CASCADE;\n",
    "                CREATE TABLE rdl.sellers (\n",
    "                    seller_id VARCHAR(50) PRIMARY KEY,\n",
    "                    seller_zip_code_prefix INTEGER,\n",
    "                    seller_city VARCHAR(100),\n",
    "                    seller_state VARCHAR(2),\n",
    "                    \n",
    "                    -- RDL additions\n",
    "                    rdl_load_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    rdl_source_system VARCHAR(50) DEFAULT 'olist',\n",
    "                    rdl_is_valid BOOLEAN DEFAULT TRUE,\n",
    "                    \n",
    "                    -- Geocoding\n",
    "                    rdl_latitude DECIMAL(10,8),\n",
    "                    rdl_longitude DECIMAL(11,8),\n",
    "                    rdl_geocoding_status VARCHAR(20),\n",
    "                    rdl_geocoding_confidence DECIMAL(3,2),  -- Aggiunta questa colonna\n",
    "                    \n",
    "                    -- Business metrics (to be calculated)\n",
    "                    rdl_total_orders INTEGER,\n",
    "                    rdl_total_revenue DECIMAL(12,2),\n",
    "                    rdl_avg_rating DECIMAL(3,2),\n",
    "                    rdl_active_status VARCHAR(20) -- active, inactive, new\n",
    "                );\n",
    "                \n",
    "                -- RDL Geolocation (normalized and cleaned)\n",
    "                DROP TABLE IF EXISTS rdl.geolocation CASCADE;\n",
    "                CREATE TABLE rdl.geolocation (\n",
    "                    geolocation_zip_code_prefix INTEGER,\n",
    "                    geolocation_lat DECIMAL(10,8),\n",
    "                    geolocation_lng DECIMAL(11,8),\n",
    "                    geolocation_city VARCHAR(100),\n",
    "                    geolocation_state VARCHAR(2),\n",
    "                    \n",
    "                    -- RDL additions\n",
    "                    rdl_load_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    rdl_is_primary BOOLEAN, -- Primary location for this ZIP\n",
    "                    rdl_confidence_score DECIMAL(3,2),\n",
    "                    rdl_region VARCHAR(50),\n",
    "                    \n",
    "                    PRIMARY KEY (geolocation_zip_code_prefix, geolocation_lat, geolocation_lng)\n",
    "                );\n",
    "                \n",
    "                -- =====================================================\n",
    "                -- RDL METADATA TABLES\n",
    "                -- =====================================================\n",
    "                \n",
    "                -- Data Quality Monitoring\n",
    "                DROP TABLE IF EXISTS rdl.data_quality_log CASCADE;\n",
    "                CREATE TABLE rdl.data_quality_log (\n",
    "                    log_id SERIAL PRIMARY KEY,\n",
    "                    check_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    table_name VARCHAR(50),\n",
    "                    check_type VARCHAR(50), -- completeness, accuracy, consistency, timeliness\n",
    "                    total_records INTEGER,\n",
    "                    valid_records INTEGER,\n",
    "                    invalid_records INTEGER,\n",
    "                    quality_score DECIMAL(5,2),\n",
    "                    error_details JSONB\n",
    "                );\n",
    "                \n",
    "                -- ETL Process Log\n",
    "                DROP TABLE IF EXISTS rdl.etl_process_log CASCADE;\n",
    "                CREATE TABLE rdl.etl_process_log (\n",
    "                    process_id SERIAL PRIMARY KEY,\n",
    "                    process_name VARCHAR(100),\n",
    "                    process_type VARCHAR(50), -- extract, transform, load\n",
    "                    start_timestamp TIMESTAMP,\n",
    "                    end_timestamp TIMESTAMP,\n",
    "                    status VARCHAR(20), -- running, completed, failed\n",
    "                    records_processed INTEGER,\n",
    "                    records_rejected INTEGER,\n",
    "                    error_message TEXT,\n",
    "                    process_metadata JSONB\n",
    "                );\n",
    "                \n",
    "                -- Data Lineage\n",
    "                DROP TABLE IF EXISTS rdl.data_lineage CASCADE;\n",
    "                CREATE TABLE rdl.data_lineage (\n",
    "                    lineage_id SERIAL PRIMARY KEY,\n",
    "                    source_system VARCHAR(50),\n",
    "                    source_table VARCHAR(100),\n",
    "                    source_column VARCHAR(100),\n",
    "                    rdl_table VARCHAR(100),\n",
    "                    rdl_column VARCHAR(100),\n",
    "                    transformation_rule TEXT,\n",
    "                    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                );\n",
    "                \n",
    "                -- =====================================================\n",
    "                -- INDEXES FOR PERFORMANCE\n",
    "                -- =====================================================\n",
    "                CREATE INDEX idx_rdl_orders_customer ON rdl.orders(customer_id);\n",
    "                CREATE INDEX idx_rdl_orders_status ON rdl.orders(order_status);\n",
    "                CREATE INDEX idx_rdl_orders_timestamp ON rdl.orders(order_purchase_timestamp);\n",
    "                \n",
    "                CREATE INDEX idx_rdl_customers_state ON rdl.customers(customer_state);\n",
    "                CREATE INDEX idx_rdl_customers_valid ON rdl.customers(rdl_is_valid);\n",
    "                \n",
    "                CREATE INDEX idx_rdl_products_category ON rdl.products(product_category_name);\n",
    "                CREATE INDEX idx_rdl_products_valid ON rdl.products(rdl_is_valid);\n",
    "                \n",
    "                CREATE INDEX idx_rdl_order_items_order ON rdl.order_items(order_id);\n",
    "                CREATE INDEX idx_rdl_order_items_product ON rdl.order_items(product_id);\n",
    "                CREATE INDEX idx_rdl_order_items_seller ON rdl.order_items(seller_id);\n",
    "                \n",
    "                -- =====================================================\n",
    "                -- VIEWS FOR DATA QUALITY MONITORING\n",
    "                -- =====================================================\n",
    "                \n",
    "                -- Overall data quality dashboard\n",
    "                CREATE OR REPLACE VIEW rdl.v_data_quality_summary AS\n",
    "                SELECT \n",
    "                    'orders' as table_name,\n",
    "                    COUNT(*) as total_records,\n",
    "                    SUM(CASE WHEN rdl_is_valid THEN 1 ELSE 0 END) as valid_records,\n",
    "                    AVG(CASE WHEN rdl_is_valid THEN 1 ELSE 0 END) * 100 as quality_percentage\n",
    "                FROM rdl.orders\n",
    "                UNION ALL\n",
    "                SELECT \n",
    "                    'customers' as table_name,\n",
    "                    COUNT(*) as total_records,\n",
    "                    SUM(CASE WHEN rdl_is_valid THEN 1 ELSE 0 END) as valid_records,\n",
    "                    AVG(CASE WHEN rdl_is_valid THEN 1 ELSE 0 END) * 100 as quality_percentage\n",
    "                FROM rdl.customers\n",
    "                UNION ALL\n",
    "                SELECT \n",
    "                    'products' as table_name,\n",
    "                    COUNT(*) as total_records,\n",
    "                    SUM(CASE WHEN rdl_is_valid THEN 1 ELSE 0 END) as valid_records,\n",
    "                    AVG(CASE WHEN rdl_is_valid THEN 1 ELSE 0 END) * 100 as quality_percentage\n",
    "                FROM rdl.products;\n",
    "                \n",
    "                -- Geocoding status view\n",
    "                CREATE OR REPLACE VIEW rdl.v_geocoding_status AS\n",
    "                SELECT \n",
    "                    'customers' as entity_type,\n",
    "                    rdl_geocoding_status,\n",
    "                    COUNT(*) as count\n",
    "                FROM rdl.customers\n",
    "                WHERE rdl_geocoding_status IS NOT NULL\n",
    "                GROUP BY rdl_geocoding_status\n",
    "                UNION ALL\n",
    "                SELECT \n",
    "                    'sellers' as entity_type,\n",
    "                    rdl_geocoding_status,\n",
    "                    COUNT(*) as count\n",
    "                FROM rdl.sellers\n",
    "                WHERE rdl_geocoding_status IS NOT NULL\n",
    "                GROUP BY rdl_geocoding_status;\n",
    "                \n",
    "                -- =====================================================\n",
    "                -- COMMENTS FOR DOCUMENTATION\n",
    "                -- =====================================================\n",
    "                COMMENT ON SCHEMA rdl IS 'Reconciled Data Layer - Intermediate storage for data cleaning, validation, and enrichment';\n",
    "                COMMENT ON TABLE rdl.orders IS 'Reconciled orders data with validation flags and calculated fields';\n",
    "                COMMENT ON TABLE rdl.customers IS 'Reconciled customer data with geocoding and quality scores';\n",
    "                COMMENT ON TABLE rdl.products IS 'Reconciled product data with translations and categorizations';\n",
    "                COMMENT ON TABLE rdl.data_quality_log IS 'Tracks data quality metrics over time';\n",
    "                COMMENT ON TABLE rdl.etl_process_log IS 'Logs all ETL processes for monitoring and debugging';\n",
    "                COMMENT ON TABLE rdl.data_lineage IS 'Tracks data flow from source to RDL to DW';\"\"\"))\n",
    "            \n",
    "            conn.commit()\n",
    "            print(\"✅ Tabelle RDL ricreate\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Errore pulizia completa: {e}\")\n",
    "        raise\n",
    "\n",
    "# Funzione specifica per caricare geolocation\n",
    "def load_geolocation_safely(df):\n",
    "    \"\"\"Carica geolocation gestendo i duplicati in modo sicuro\"\"\"\n",
    "    print(\"\\n📍 Caricamento speciale GEOLOCATION...\")\n",
    "    \n",
    "    # Prepara i dati\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Assicura tipi corretti e precisione\n",
    "    df['geolocation_zip_code_prefix'] = df['geolocation_zip_code_prefix'].astype(int)\n",
    "    df['geolocation_lat'] = df['geolocation_lat'].astype(float).round(8)\n",
    "    df['geolocation_lng'] = df['geolocation_lng'].astype(float).round(8)\n",
    "    \n",
    "    # Rimuovi duplicati\n",
    "    print(f\"   Record originali: {len(df):,}\")\n",
    "    df = df.drop_duplicates(\n",
    "        subset=['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng'],\n",
    "        keep='first'\n",
    "    )\n",
    "    print(f\"   Record dopo deduplicazione: {len(df):,}\")\n",
    "    \n",
    "    # Aggiungi campi RDL\n",
    "    df['rdl_load_timestamp'] = datetime.now()\n",
    "    df['rdl_is_primary'] = True\n",
    "    df['rdl_confidence_score'] = 1.0\n",
    "    \n",
    "    # Mappa regioni\n",
    "    region_map = {\n",
    "        'SP': 'Sudeste', 'RJ': 'Sudeste', 'MG': 'Sudeste', 'ES': 'Sudeste',\n",
    "        'PR': 'Sul', 'SC': 'Sul', 'RS': 'Sul',\n",
    "        'BA': 'Nordeste', 'PE': 'Nordeste', 'CE': 'Nordeste', 'PB': 'Nordeste',\n",
    "        'RN': 'Nordeste', 'SE': 'Nordeste', 'AL': 'Nordeste', 'MA': 'Nordeste', 'PI': 'Nordeste',\n",
    "        'GO': 'Centro-Oeste', 'MT': 'Centro-Oeste', 'MS': 'Centro-Oeste', 'DF': 'Centro-Oeste',\n",
    "        'AC': 'Norte', 'AP': 'Norte', 'AM': 'Norte', 'PA': 'Norte', 'RO': 'Norte', 'RR': 'Norte', 'TO': 'Norte'\n",
    "    }\n",
    "    df['rdl_region'] = df['geolocation_state'].map(region_map).fillna('Unknown')\n",
    "    \n",
    "    print(\"   Caricamento in batch...\")\n",
    "    try:\n",
    "        # Prepara le colonne nell'ordine corretto\n",
    "        columns_order = [\n",
    "            'geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng',\n",
    "            'geolocation_city', 'geolocation_state', 'rdl_load_timestamp',\n",
    "            'rdl_is_primary', 'rdl_confidence_score', 'rdl_region'\n",
    "        ]\n",
    "        df_final = df[columns_order]\n",
    "        \n",
    "        # Carica in batch usando to_sql\n",
    "        df_final.to_sql(\n",
    "            'geolocation', \n",
    "            engine, \n",
    "            schema='rdl',\n",
    "            if_exists='append', \n",
    "            index=False, \n",
    "            method='multi',\n",
    "            chunksize=10000  # Processa 10000 record alla volta\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ GEOLOCATION caricata: {len(df_final):,} record\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Errore caricamento: {e}\")\n",
    "        return False\n",
    "\n",
    "def load_to_rdl_fixed(df, table_name):\n",
    "    \"\"\"Carica dati in RDL con gestione duplicati\"\"\"\n",
    "    try:\n",
    "        df = df.copy()\n",
    "        \n",
    "        # GESTIONE SPECIALE PER REVIEWS - duplicati su review_id\n",
    "        if table_name == 'reviews':\n",
    "            original_count = len(df)\n",
    "            df = df.drop_duplicates(subset=['review_id'], keep='first')\n",
    "            if original_count > len(df):\n",
    "                print(f\"   ⚠️ Rimossi {original_count - len(df)} reviews duplicati\")\n",
    "        \n",
    "        # GESTIONE SPECIALE PER GEOLOCATION - duplicati sulla chiave composita\n",
    "        if table_name == 'geolocation':\n",
    "            original_count = len(df)\n",
    "            df = df.drop_duplicates(\n",
    "                subset=['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng'], \n",
    "                keep='first'\n",
    "            )\n",
    "            if original_count > len(df):\n",
    "                print(f\"   ⚠️ Rimossi {original_count - len(df)} geolocation duplicati\")\n",
    "        \n",
    "        # Aggiungi campi RDL comuni\n",
    "        df['rdl_load_timestamp'] = datetime.now()\n",
    "        df['rdl_source_system'] = 'olist'\n",
    "        df['rdl_is_valid'] = True\n",
    "        \n",
    "        # Aggiungi campi specifici per tabella\n",
    "        if table_name == 'orders':\n",
    "            df['rdl_validation_errors'] = ''\n",
    "            df['rdl_processing_status'] = 'new'\n",
    "            # Calcola campi derivati se possibile\n",
    "            if 'order_delivered_customer_date' in df.columns and 'order_estimated_delivery_date' in df.columns:\n",
    "                df['order_delivered_customer_date'] = pd.to_datetime(df['order_delivered_customer_date'], errors='coerce')\n",
    "                df['order_estimated_delivery_date'] = pd.to_datetime(df['order_estimated_delivery_date'], errors='coerce')\n",
    "                mask_valid = df['order_delivered_customer_date'].notna() & df['order_estimated_delivery_date'].notna()\n",
    "                df.loc[mask_valid, 'rdl_delivery_delay_days'] = (\n",
    "                    df.loc[mask_valid, 'order_delivered_customer_date'] - \n",
    "                    df.loc[mask_valid, 'order_estimated_delivery_date']\n",
    "                ).dt.days\n",
    "            \n",
    "        elif table_name == 'customers':\n",
    "            df['rdl_validation_errors'] = ''\n",
    "            df['rdl_geocoding_status'] = 'not_geocoded'\n",
    "            df['rdl_geocoding_confidence'] = 0.0\n",
    "            df['rdl_address_completeness_score'] = 1.0\n",
    "            df['rdl_data_quality_score'] = 1.0\n",
    "            \n",
    "        elif table_name == 'products':\n",
    "            df['rdl_validation_errors'] = ''\n",
    "            df['rdl_has_complete_dimensions'] = True\n",
    "            df['rdl_has_photos'] = df['product_photos_qty'] > 0\n",
    "            df['rdl_description_quality'] = 'medium'\n",
    "            df['rdl_weight_category'] = 'medium'\n",
    "            df['rdl_size_category'] = 'medium'\n",
    "            \n",
    "        elif table_name == 'order_items':\n",
    "            # Calcola totale\n",
    "            if 'price' in df.columns and 'freight_value' in df.columns:\n",
    "                df['rdl_total_item_value'] = df['price'] + df['freight_value']\n",
    "                df['rdl_freight_percentage'] = (df['freight_value'] / df['price'].replace(0, 0.01) * 100).round(2)\n",
    "                \n",
    "        elif table_name == 'payments':\n",
    "            df['rdl_is_multi_payment'] = False\n",
    "            df['rdl_payment_method_count'] = 1\n",
    "            \n",
    "        elif table_name == 'reviews':\n",
    "            df['rdl_has_comment'] = df['review_comment_message'].notna()\n",
    "            df['rdl_comment_length'] = df['review_comment_message'].fillna('').str.len()\n",
    "            \n",
    "        elif table_name == 'sellers':\n",
    "            df['rdl_geocoding_status'] = 'not_geocoded'\n",
    "            df['rdl_geocoding_confidence'] = 0.0\n",
    "            df['rdl_active_status'] = 'active'\n",
    "            \n",
    "        elif table_name == 'geolocation':\n",
    "            df['rdl_is_primary'] = True\n",
    "            df['rdl_confidence_score'] = 1.0\n",
    "            # Mappa regioni\n",
    "            region_map = {\n",
    "                'SP': 'Sudeste', 'RJ': 'Sudeste', 'MG': 'Sudeste', 'ES': 'Sudeste',\n",
    "                'PR': 'Sul', 'SC': 'Sul', 'RS': 'Sul',\n",
    "                'BA': 'Nordeste', 'PE': 'Nordeste', 'CE': 'Nordeste', 'PB': 'Nordeste',\n",
    "                'RN': 'Nordeste', 'SE': 'Nordeste', 'AL': 'Nordeste', 'MA': 'Nordeste', 'PI': 'Nordeste',\n",
    "                'GO': 'Centro-Oeste', 'MT': 'Centro-Oeste', 'MS': 'Centro-Oeste', 'DF': 'Centro-Oeste',\n",
    "                'AC': 'Norte', 'AP': 'Norte', 'AM': 'Norte', 'PA': 'Norte', 'RO': 'Norte', 'RR': 'Norte', 'TO': 'Norte'\n",
    "            }\n",
    "            df['rdl_region'] = df['geolocation_state'].map(region_map).fillna('Unknown')\n",
    "        \n",
    "        # Ottieni colonne esistenti nel database\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(f\"\"\"\n",
    "                SELECT column_name \n",
    "                FROM information_schema.columns \n",
    "                WHERE table_schema = 'rdl' AND table_name = :table_name\n",
    "            \"\"\"), {\"table_name\": table_name})\n",
    "            existing_columns = [row[0] for row in result]\n",
    "        \n",
    "        # Mantieni solo colonne che esistono\n",
    "        columns_to_keep = [col for col in df.columns if col in existing_columns]\n",
    "        df_final = df[columns_to_keep]\n",
    "        \n",
    "        # Carica nel database\n",
    "        df_final.to_sql(table_name, engine, schema='rdl', if_exists='append', index=False, method='multi', chunksize=1000)\n",
    "        print(f\"✅ {table_name}: {len(df_final):,} record caricati\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {table_name}: Errore - {e}\")\n",
    "        return False\n",
    "\n",
    "# ESECUZIONE PRINCIPALE\n",
    "# 1. Pulisci completamente (opzionale, solo se necessario)\n",
    "if False:  # Cambia in False se non vuoi ripulire tutto\n",
    "    clean_rdl_completely()\n",
    "    \n",
    "    # Dopo la pulizia, devi rieseguire create_rdl_schema.sql\n",
    "    print(\"\\n⚠️ ATTENZIONE: Devi rieseguire create_rdl_schema.sql per ricreare tutte le tabelle!\")\n",
    "    print(\"In pgAdmin, esegui il file 2_rdl_layer/create_rdl_schema.sql\")\n",
    "    print(\"Poi riprendi da qui.\")\n",
    "    # Uncomment per continuare automaticamente se hai il file\n",
    "    # exec(open('2_rdl_layer/create_rdl_schema.sql').read())\n",
    "\n",
    "# 2. Carica prima geolocation con il metodo sicuro\n",
    "load_geolocation_safely(source_data['geolocation'])\n",
    "\n",
    "# 3. Poi carica le altre tabelle normalmente\n",
    "print(\"\\n📥 Caricamento altre tabelle RDL...\")\n",
    "for table_name in ['orders', 'customers', 'products', 'order_items', 'payments', 'reviews', 'sellers']:\n",
    "    if table_name in source_data:\n",
    "        # Usa la tua funzione esistente per le altre tabelle\n",
    "        load_to_rdl_fixed(source_data[table_name], table_name)\n",
    "\n",
    "# Report finale\n",
    "print(\"\\n📊 REPORT FINALE RDL:\")\n",
    "print(\"-\"*60)\n",
    "with engine.connect() as conn:\n",
    "    for table in ['orders', 'customers', 'products', 'order_items', 'payments', 'reviews', 'sellers', 'geolocation']:\n",
    "        try:\n",
    "            count = conn.execute(text(f\"SELECT COUNT(*) FROM rdl.{table}\")).scalar()\n",
    "            print(f\"rdl.{table}: {count:,} record\")\n",
    "        except Exception as e:\n",
    "            print(f\"rdl.{table}: ERRORE - {e}\")\n",
    "\n",
    "print(\"\\n✅ FASE 2 completata con successo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b297e18f-7d51-4d67-b20c-bfea4738d2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 FASE 3: TRANSFORM & LOAD VERSO DW\n",
      "------------------------------------------------------------\n",
      "🧹 Pulizia tabelle DW...\n",
      "✅ Tabelle DW pulite\n",
      "\n",
      "📌 DIM_GEOGRAPHY...\n",
      "   ZIP codes da caricare: 19,015\n",
      "✅ Caricati 19,015 ZIP codes\n",
      "   Chiavi caricate: 19,015\n",
      "\n",
      "📌 DIM_CUSTOMER...\n",
      "✅ Caricati 99,441 customers\n",
      "\n",
      "📌 DIM_PRODUCT...\n",
      "✅ Caricati 32,951 products\n",
      "\n",
      "📌 DIM_SELLER...\n",
      "✅ Caricati 3,095 sellers\n",
      "\n",
      "📌 DIM_TIME...\n",
      "✅ Caricati 634 giorni\n",
      "\n",
      "📌 DIM_PAYMENT...\n",
      "✅ Caricati 28 metodi di pagamento\n",
      "\n",
      "📌 FACT_SALES...\n",
      "   🔑 Caricamento chiavi dimensioni...\n",
      "   Record da processare: 110,197\n",
      "   ✓ Processati 25,000/110,197 record (22.7%)\n",
      "   ✓ Processati 50,000/110,197 record (45.4%)\n",
      "   ✓ Processati 75,000/110,197 record (68.1%)\n",
      "   ✓ Processati 100,000/110,197 record (90.7%)\n",
      "   ✓ Processati 115,358/110,197 record (104.7%)\n",
      "✅ Caricati 115,358 record in fact_sales\n",
      "\n",
      "📊 VERIFICA CARICAMENTO DW:\n",
      "------------------------------------------------------------\n",
      "dim_geography: 19,015 ZIP codes\n",
      "dim_customer: 99,441 clienti\n",
      "dim_product: 32,951 prodotti\n",
      "dim_seller: 3,095 venditori\n",
      "dim_time: 634 date\n",
      "dim_payment: 28 metodi pagamento\n",
      "fact_sales: 115,358 vendite\n",
      "------------------------------------------------------------\n",
      "TOTALE RECORD NEL DW: 270,522\n",
      "\n",
      "✅ FASE 3 completata con successo!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FASE 3: TRANSFORM & LOAD VERSO DW\n",
    "# =============================================================================\n",
    "print(\"\\n🚀 FASE 3: TRANSFORM & LOAD VERSO DW\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Prima pulisci le tabelle DW esistenti\n",
    "print(\"🧹 Pulizia tabelle DW...\")\n",
    "with engine.connect() as conn:\n",
    "    # Pulisci in ordine per rispettare le FK\n",
    "    conn.execute(text(\"TRUNCATE TABLE fact_sales CASCADE\"))\n",
    "    conn.execute(text(\"TRUNCATE TABLE dim_customer CASCADE\"))\n",
    "    conn.execute(text(\"TRUNCATE TABLE dim_product CASCADE\"))\n",
    "    conn.execute(text(\"TRUNCATE TABLE dim_seller CASCADE\"))\n",
    "    conn.execute(text(\"TRUNCATE TABLE dim_time CASCADE\"))\n",
    "    conn.execute(text(\"TRUNCATE TABLE dim_payment CASCADE\"))\n",
    "    conn.execute(text(\"TRUNCATE TABLE dim_geography CASCADE\"))\n",
    "    conn.commit()\n",
    "print(\"✅ Tabelle DW pulite\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CARICA DIM_GEOGRAPHY (necessaria per le altre dimensioni)\n",
    "# =============================================================================\n",
    "print(\"\\n📌 DIM_GEOGRAPHY...\")\n",
    "\n",
    "# METODO 1: Carica direttamente nel DB senza passare da pandas\n",
    "with engine.connect() as conn:\n",
    "    # Prima conta quanti record\n",
    "    count_query = \"\"\"\n",
    "        SELECT COUNT(DISTINCT geolocation_zip_code_prefix) \n",
    "        FROM rdl.geolocation \n",
    "        WHERE rdl_is_primary = true\n",
    "    \"\"\"\n",
    "    total_zips = conn.execute(text(count_query)).scalar()\n",
    "    print(f\"   ZIP codes da caricare: {total_zips:,}\")\n",
    "    \n",
    "    # Poi inserisci direttamente con SQL\n",
    "    insert_query = \"\"\"\n",
    "        INSERT INTO dim_geography (zip_code_prefix, city, state, latitude, longitude, region)\n",
    "        SELECT DISTINCT\n",
    "            geolocation_zip_code_prefix::varchar,\n",
    "            geolocation_city,\n",
    "            geolocation_state,\n",
    "            geolocation_lat,\n",
    "            geolocation_lng,\n",
    "            rdl_region\n",
    "        FROM rdl.geolocation\n",
    "        WHERE rdl_is_primary = true\n",
    "        ON CONFLICT (zip_code_prefix) DO NOTHING\n",
    "    \"\"\"\n",
    "    \n",
    "    result = conn.execute(text(insert_query))\n",
    "    conn.commit()\n",
    "    print(f\"✅ Caricati {result.rowcount:,} ZIP codes\")\n",
    "\n",
    "# Carica solo le chiavi per i join successivi (molto più leggero)\n",
    "dim_geography_keys = pd.read_sql(\n",
    "    \"SELECT geography_key, zip_code_prefix FROM dim_geography\", \n",
    "    engine\n",
    ")\n",
    "print(f\"   Chiavi caricate: {len(dim_geography_keys):,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. CARICA DIM_CUSTOMER\n",
    "# =============================================================================\n",
    "print(\"\\n📌 DIM_CUSTOMER...\")\n",
    "query_customer = \"\"\"\n",
    "    SELECT DISTINCT\n",
    "        customer_id,\n",
    "        customer_city,\n",
    "        customer_state,\n",
    "        customer_zip_code_prefix::varchar as customer_zip_code_prefix,\n",
    "        rdl_latitude as customer_latitude,\n",
    "        rdl_longitude as customer_longitude\n",
    "    FROM rdl.customers\n",
    "    WHERE rdl_is_valid = true\n",
    "\"\"\"\n",
    "dim_customer = pd.read_sql(query_customer, engine)\n",
    "\n",
    "# Aggiungi geography_key\n",
    "dim_customer = dim_customer.merge(\n",
    "    dim_geography_keys,\n",
    "    left_on='customer_zip_code_prefix',\n",
    "    right_on='zip_code_prefix',\n",
    "    how='left'\n",
    ")\n",
    "dim_customer = dim_customer.drop('zip_code_prefix', axis=1)\n",
    "\n",
    "# Carica in DW\n",
    "dim_customer.to_sql('dim_customer', engine, if_exists='append', index=False, method='multi')\n",
    "print(f\"✅ Caricati {len(dim_customer):,} customers\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. CARICA DIM_PRODUCT\n",
    "# =============================================================================\n",
    "print(\"\\n📌 DIM_PRODUCT...\")\n",
    "query_product = \"\"\"\n",
    "    SELECT DISTINCT\n",
    "        product_id,\n",
    "        product_category_name,\n",
    "        COALESCE(rdl_category_english, product_category_name) as product_category_name_english,\n",
    "        product_name_lenght,\n",
    "        product_description_lenght,\n",
    "        product_photos_qty,\n",
    "        product_weight_g,\n",
    "        product_length_cm,\n",
    "        product_height_cm,\n",
    "        product_width_cm,\n",
    "        rdl_weight_category as weight_category,\n",
    "        rdl_size_category as size_category\n",
    "    FROM rdl.products\n",
    "    WHERE rdl_is_valid = true\n",
    "\"\"\"\n",
    "dim_product = pd.read_sql(query_product, engine)\n",
    "dim_product.to_sql('dim_product', engine, if_exists='append', index=False, method='multi')\n",
    "print(f\"✅ Caricati {len(dim_product):,} products\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. CARICA DIM_SELLER\n",
    "# =============================================================================\n",
    "print(\"\\n📌 DIM_SELLER...\")\n",
    "query_seller = \"\"\"\n",
    "    SELECT DISTINCT\n",
    "        seller_id,\n",
    "        seller_city,\n",
    "        seller_state,\n",
    "        seller_zip_code_prefix::varchar as seller_zip_code_prefix,\n",
    "        rdl_latitude as seller_latitude,\n",
    "        rdl_longitude as seller_longitude\n",
    "    FROM rdl.sellers\n",
    "    WHERE rdl_is_valid = true\n",
    "\"\"\"\n",
    "dim_seller = pd.read_sql(query_seller, engine)\n",
    "\n",
    "# Aggiungi geography_key\n",
    "dim_seller = dim_seller.merge(\n",
    "    dim_geography_keys,\n",
    "    left_on='seller_zip_code_prefix',\n",
    "    right_on='zip_code_prefix',\n",
    "    how='left'\n",
    ")\n",
    "dim_seller = dim_seller.drop('zip_code_prefix', axis=1)\n",
    "\n",
    "# Carica in DW\n",
    "dim_seller.to_sql('dim_seller', engine, if_exists='append', index=False, method='multi')\n",
    "print(f\"✅ Caricati {len(dim_seller):,} sellers\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. CARICA DIM_TIME\n",
    "# =============================================================================\n",
    "print(\"\\n📌 DIM_TIME...\")\n",
    "query_time = \"\"\"\n",
    "    SELECT DISTINCT\n",
    "        DATE(order_purchase_timestamp) as full_date\n",
    "    FROM rdl.orders\n",
    "    WHERE order_purchase_timestamp IS NOT NULL\n",
    "\"\"\"\n",
    "dates_df = pd.read_sql(query_time, engine)\n",
    "\n",
    "# Crea attributi temporali\n",
    "dim_time = pd.DataFrame()\n",
    "dim_time['full_date'] = pd.to_datetime(dates_df['full_date'])\n",
    "dim_time['day_of_week'] = dim_time['full_date'].dt.dayofweek\n",
    "dim_time['day_name'] = dim_time['full_date'].dt.day_name()\n",
    "dim_time['day_of_month'] = dim_time['full_date'].dt.day\n",
    "dim_time['month_num'] = dim_time['full_date'].dt.month\n",
    "dim_time['month_name'] = dim_time['full_date'].dt.month_name()\n",
    "dim_time['quarter'] = dim_time['full_date'].dt.quarter\n",
    "dim_time['year'] = dim_time['full_date'].dt.year\n",
    "dim_time['is_weekend'] = dim_time['day_of_week'].isin([5, 6])\n",
    "\n",
    "# Aggiungi stagione brasiliana\n",
    "def get_season_brazil(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Verão'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Outono'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Inverno'\n",
    "    else:\n",
    "        return 'Primavera'\n",
    "\n",
    "dim_time['season_brazil'] = dim_time['month_num'].apply(get_season_brazil)\n",
    "\n",
    "# Carica in DW\n",
    "dim_time.to_sql('dim_time', engine, if_exists='append', index=False, method='multi')\n",
    "print(f\"✅ Caricati {len(dim_time):,} giorni\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. CARICA DIM_PAYMENT\n",
    "# =============================================================================\n",
    "print(\"\\n📌 DIM_PAYMENT...\")\n",
    "query_payment = \"\"\"\n",
    "    SELECT DISTINCT\n",
    "        payment_type,\n",
    "        payment_installments,\n",
    "        CASE \n",
    "            WHEN payment_installments = 1 THEN 'Cash/Single Payment'\n",
    "            WHEN payment_installments <= 6 THEN 'Short Term'\n",
    "            WHEN payment_installments <= 12 THEN 'Medium Term'\n",
    "            ELSE 'Long Term'\n",
    "        END as installment_category\n",
    "    FROM rdl.payments\n",
    "    WHERE payment_type IS NOT NULL\n",
    "\"\"\"\n",
    "dim_payment = pd.read_sql(query_payment, engine)\n",
    "dim_payment.to_sql('dim_payment', engine, if_exists='append', index=False, method='multi')\n",
    "print(f\"✅ Caricati {len(dim_payment):,} metodi di pagamento\")\n",
    "\n",
    "# =============================================================================\n",
    "# 7. CARICA FACT_SALES\n",
    "# =============================================================================\n",
    "print(\"\\n📌 FACT_SALES...\")\n",
    "\n",
    "# METODO OTTIMIZZATO: Processa in chunk\n",
    "print(\"   🔑 Caricamento chiavi dimensioni...\")\n",
    "dim_customer_keys = pd.read_sql(\"SELECT customer_key, customer_id, customer_state FROM dim_customer\", engine)\n",
    "dim_product_keys = pd.read_sql(\"SELECT product_key, product_id FROM dim_product\", engine)\n",
    "dim_seller_keys = pd.read_sql(\"SELECT seller_key, seller_id, seller_state FROM dim_seller\", engine)\n",
    "dim_time_keys = pd.read_sql(\"SELECT time_key, full_date FROM dim_time\", engine)\n",
    "dim_payment_keys = pd.read_sql(\"SELECT payment_key, payment_type, payment_installments FROM dim_payment\", engine)\n",
    "\n",
    "# Conta totale record da processare\n",
    "with engine.connect() as conn:\n",
    "    total_facts = conn.execute(text(\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM rdl.orders o\n",
    "        JOIN rdl.order_items oi ON o.order_id = oi.order_id\n",
    "        WHERE o.order_status = 'delivered' AND o.rdl_is_valid = true\n",
    "    \"\"\")).scalar()\n",
    "    print(f\"   Record da processare: {total_facts:,}\")\n",
    "\n",
    "# Processa in chunk usando LIMIT/OFFSET\n",
    "chunk_size = 25000\n",
    "processed = 0\n",
    "\n",
    "for offset in range(0, total_facts, chunk_size):\n",
    "    # Query con LIMIT/OFFSET\n",
    "    query_facts = f\"\"\"\n",
    "        SELECT \n",
    "            o.order_id,\n",
    "            oi.order_item_id,\n",
    "            o.customer_id,\n",
    "            oi.product_id,\n",
    "            oi.seller_id,\n",
    "            DATE(o.order_purchase_timestamp) as order_date,\n",
    "            p.payment_type,\n",
    "            p.payment_installments,\n",
    "            oi.price,\n",
    "            oi.freight_value,\n",
    "            1 as quantity,\n",
    "            p.payment_value,\n",
    "            r.review_score,\n",
    "            oi.rdl_total_item_value as total_item_value\n",
    "        FROM rdl.orders o\n",
    "        JOIN rdl.order_items oi ON o.order_id = oi.order_id\n",
    "        JOIN rdl.payments p ON o.order_id = p.order_id\n",
    "        LEFT JOIN rdl.reviews r ON o.order_id = r.order_id\n",
    "        WHERE o.order_status = 'delivered'\n",
    "            AND o.rdl_is_valid = true\n",
    "            AND oi.rdl_is_valid = true\n",
    "        LIMIT {chunk_size} OFFSET {offset}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Carica chunk\n",
    "    chunk_df = pd.read_sql(query_facts, engine)\n",
    "    \n",
    "    if len(chunk_df) == 0:\n",
    "        break\n",
    "    \n",
    "    # Join con chiavi\n",
    "    chunk_df = chunk_df.merge(dim_customer_keys, on='customer_id', how='inner')\n",
    "    chunk_df = chunk_df.merge(dim_product_keys, on='product_id', how='inner')\n",
    "    chunk_df = chunk_df.merge(dim_seller_keys, on='seller_id', how='inner')\n",
    "    chunk_df = chunk_df.merge(dim_time_keys, left_on='order_date', right_on='full_date', how='inner')\n",
    "    chunk_df = chunk_df.merge(dim_payment_keys, on=['payment_type', 'payment_installments'], how='inner')\n",
    "    \n",
    "    # Calcola metriche\n",
    "    chunk_df['is_cross_state_sale'] = chunk_df['customer_state'] != chunk_df['seller_state']\n",
    "    chunk_df['shipping_type'] = chunk_df.apply(\n",
    "        lambda x: 'Local' if x['customer_state'] == x['seller_state'] else 'Interstate',\n",
    "        axis=1\n",
    "    )\n",
    "    chunk_df['freight_percentage'] = (chunk_df['freight_value'] / chunk_df['price'].replace(0, 0.01) * 100).round(2).clip(upper=999.99)\n",
    "    chunk_df['net_revenue'] = chunk_df['price'] - chunk_df['freight_value']\n",
    "    chunk_df['customer_seller_distance_km'] = None\n",
    "    \n",
    "    # Seleziona colonne finali\n",
    "    columns_final = [\n",
    "        'customer_key', 'product_key', 'time_key', 'seller_key', 'payment_key',\n",
    "        'order_id', 'order_item_id',\n",
    "        'price', 'freight_value', 'quantity', 'payment_value', 'review_score',\n",
    "        'total_item_value', 'customer_seller_distance_km', 'is_cross_state_sale',\n",
    "        'shipping_type', 'freight_percentage', 'net_revenue'\n",
    "    ]\n",
    "    \n",
    "    chunk_final = chunk_df[columns_final]\n",
    "    \n",
    "    # Carica chunk\n",
    "    chunk_final.to_sql('fact_sales', engine, if_exists='append', index=False, method='multi')\n",
    "    \n",
    "    processed += len(chunk_df)\n",
    "    print(f\"   ✓ Processati {processed:,}/{total_facts:,} record ({processed/total_facts*100:.1f}%)\")\n",
    "    \n",
    "    # Libera memoria\n",
    "    del chunk_df\n",
    "    del chunk_final\n",
    "\n",
    "print(f\"✅ Caricati {processed:,} record in fact_sales\")\n",
    "\n",
    "# =============================================================================\n",
    "# VERIFICA FINALE FASE 3\n",
    "# =============================================================================\n",
    "print(\"\\n📊 VERIFICA CARICAMENTO DW:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    tables = [\n",
    "        ('dim_geography', 'ZIP codes'),\n",
    "        ('dim_customer', 'clienti'),\n",
    "        ('dim_product', 'prodotti'),\n",
    "        ('dim_seller', 'venditori'),\n",
    "        ('dim_time', 'date'),\n",
    "        ('dim_payment', 'metodi pagamento'),\n",
    "        ('fact_sales', 'vendite')\n",
    "    ]\n",
    "    \n",
    "    total = 0\n",
    "    for table, desc in tables:\n",
    "        count = conn.execute(text(f\"SELECT COUNT(*) FROM {table}\")).scalar()\n",
    "        print(f\"{table}: {count:,} {desc}\")\n",
    "        total += count\n",
    "    \n",
    "    print(\"-\"*60)\n",
    "    print(f\"TOTALE RECORD NEL DW: {total:,}\")\n",
    "\n",
    "print(\"\\n✅ FASE 3 completata con successo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22a2897d-2428-4f2f-8f1b-7b9977ac27e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tabella data_quality_log pulita\n",
      "\n",
      "📊 Registrazione metriche qualità...\n",
      "✅ Metriche qualità registrate\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FUNZIONE PER REGISTRARE METRICHE QUALITÀ\n",
    "# =============================================================================\n",
    "def log_data_quality_metrics(engine):\n",
    "    \"\"\"Registra le metriche di qualità dei dati in RDL\"\"\"\n",
    "    print(\"\\n📊 Registrazione metriche qualità...\")\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        # Metriche per ogni tabella RDL (esclusa geolocation che non ha rdl_is_valid)\n",
    "        tables_to_check = [\n",
    "            'orders', 'customers', 'products', 'order_items', \n",
    "            'payments', 'reviews', 'sellers'  # RIMOSSA 'geolocation'\n",
    "        ]\n",
    "        \n",
    "        for table in tables_to_check:\n",
    "            try:\n",
    "                # Inizia una nuova transazione per ogni tabella\n",
    "                trans = conn.begin()\n",
    "                \n",
    "                # Conta record totali e validi\n",
    "                query = f\"\"\"\n",
    "                    SELECT \n",
    "                        COUNT(*) as total,\n",
    "                        SUM(CASE WHEN rdl_is_valid = true THEN 1 ELSE 0 END) as valid,\n",
    "                        SUM(CASE WHEN rdl_is_valid = false THEN 1 ELSE 0 END) as invalid\n",
    "                    FROM rdl.{table}\n",
    "                \"\"\"\n",
    "                result = conn.execute(text(query)).fetchone()\n",
    "                \n",
    "                if result and result[0] > 0:\n",
    "                    quality_score = (result[1] / result[0]) * 100 if result[0] > 0 else 0\n",
    "                    \n",
    "                    # Inserisci in data_quality_log\n",
    "                    insert_query = \"\"\"\n",
    "                        INSERT INTO rdl.data_quality_log \n",
    "                        (table_name, check_type, total_records, valid_records, \n",
    "                         invalid_records, quality_score, error_details)\n",
    "                        VALUES (:table, 'validation', :total, :valid, :invalid, :score, :details)\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    conn.execute(text(insert_query), {\n",
    "                        'table': table,\n",
    "                        'total': int(result[0]),\n",
    "                        'valid': int(result[1]),\n",
    "                        'invalid': int(result[2]),\n",
    "                        'score': float(quality_score),\n",
    "                        'details': '{\"source\": \"main_etl_pipeline\"}'\n",
    "                    })\n",
    "                \n",
    "                trans.commit()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️ Errore metriche {table}: {e}\")\n",
    "                if trans:\n",
    "                    trans.rollback()\n",
    "        \n",
    "        # Metrica speciale per geolocation (senza rdl_is_valid)\n",
    "        try:\n",
    "            trans = conn.begin()\n",
    "            \n",
    "            geo_query = \"\"\"\n",
    "                SELECT COUNT(*) as total,\n",
    "                       COUNT(DISTINCT geolocation_zip_code_prefix) as unique_zips\n",
    "                FROM rdl.geolocation\n",
    "            \"\"\"\n",
    "            result = conn.execute(text(geo_query)).fetchone()\n",
    "            \n",
    "            if result and result[0] > 0:\n",
    "                # Per geolocation consideriamo tutti validi\n",
    "                insert_query = \"\"\"\n",
    "                    INSERT INTO rdl.data_quality_log \n",
    "                    (table_name, check_type, total_records, valid_records, \n",
    "                     invalid_records, quality_score, error_details)\n",
    "                    VALUES ('geolocation', 'validation', :total, :valid, 0, 100.0, \n",
    "                            :details)\n",
    "                \"\"\"\n",
    "                \n",
    "                conn.execute(text(insert_query), {\n",
    "                    'total': int(result[0]),\n",
    "                    'valid': int(result[0]),  # Tutti considerati validi\n",
    "                    'details': f'{{\"unique_zips\": {result[1]}, \"source\": \"main_etl_pipeline\"}}'\n",
    "                })\n",
    "            \n",
    "            trans.commit()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Errore metriche geolocation: {e}\")\n",
    "            if trans:\n",
    "                trans.rollback()\n",
    "        \n",
    "        # Completezza indirizzi customers\n",
    "        try:\n",
    "            trans = conn.begin()\n",
    "            \n",
    "            completeness_query = \"\"\"\n",
    "                INSERT INTO rdl.data_quality_log \n",
    "                (table_name, check_type, total_records, valid_records, invalid_records, quality_score)\n",
    "                SELECT \n",
    "                    'customers',\n",
    "                    'completeness',\n",
    "                    COUNT(*),\n",
    "                    SUM(CASE WHEN customer_city IS NOT NULL \n",
    "                        AND customer_state IS NOT NULL \n",
    "                        AND customer_zip_code_prefix IS NOT NULL \n",
    "                        THEN 1 ELSE 0 END),\n",
    "                    SUM(CASE WHEN customer_city IS NULL \n",
    "                        OR customer_state IS NULL \n",
    "                        OR customer_zip_code_prefix IS NULL \n",
    "                        THEN 1 ELSE 0 END),\n",
    "                    AVG(CASE WHEN customer_city IS NOT NULL \n",
    "                        AND customer_state IS NOT NULL \n",
    "                        AND customer_zip_code_prefix IS NOT NULL \n",
    "                        THEN 100.0 ELSE 0.0 END)\n",
    "                FROM rdl.customers\n",
    "            \"\"\"\n",
    "            conn.execute(text(completeness_query))\n",
    "            trans.commit()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Errore completeness customers: {e}\")\n",
    "            if trans:\n",
    "                trans.rollback()\n",
    "        \n",
    "        # Accuratezza date orders\n",
    "        try:\n",
    "            trans = conn.begin()\n",
    "            \n",
    "            accuracy_query = \"\"\"\n",
    "                INSERT INTO rdl.data_quality_log \n",
    "                (table_name, check_type, total_records, valid_records, invalid_records, quality_score)\n",
    "                SELECT \n",
    "                    'orders',\n",
    "                    'accuracy',\n",
    "                    COUNT(*),\n",
    "                    SUM(CASE WHEN order_delivered_customer_date >= order_purchase_timestamp \n",
    "                        OR order_delivered_customer_date IS NULL THEN 1 ELSE 0 END),\n",
    "                    SUM(CASE WHEN order_delivered_customer_date < order_purchase_timestamp THEN 1 ELSE 0 END),\n",
    "                    AVG(CASE WHEN order_delivered_customer_date >= order_purchase_timestamp \n",
    "                        OR order_delivered_customer_date IS NULL THEN 100.0 ELSE 0.0 END)\n",
    "                FROM rdl.orders\n",
    "            \"\"\"\n",
    "            conn.execute(text(accuracy_query))\n",
    "            trans.commit()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Errore accuracy orders: {e}\")\n",
    "            if trans:\n",
    "                trans.rollback()\n",
    "        \n",
    "        # Consistenza order_items\n",
    "        try:\n",
    "            trans = conn.begin()\n",
    "            \n",
    "            consistency_query = \"\"\"\n",
    "                INSERT INTO rdl.data_quality_log \n",
    "                (table_name, check_type, total_records, valid_records, invalid_records, quality_score)\n",
    "                SELECT \n",
    "                    'order_items',\n",
    "                    'consistency',\n",
    "                    COUNT(*),\n",
    "                    SUM(CASE WHEN oi.order_id IN (SELECT order_id FROM rdl.orders) THEN 1 ELSE 0 END),\n",
    "                    SUM(CASE WHEN oi.order_id NOT IN (SELECT order_id FROM rdl.orders) THEN 1 ELSE 0 END),\n",
    "                    AVG(CASE WHEN oi.order_id IN (SELECT order_id FROM rdl.orders) THEN 100.0 ELSE 0.0 END)\n",
    "                FROM rdl.order_items oi\n",
    "            \"\"\"\n",
    "            conn.execute(text(consistency_query))\n",
    "            trans.commit()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Errore consistency order_items: {e}\")\n",
    "            if trans:\n",
    "                trans.rollback()\n",
    "        \n",
    "        print(\"✅ Metriche qualità registrate\")\n",
    "        \n",
    "# Pulisci la tabella data_quality_log prima di riprovare\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"DELETE FROM rdl.data_quality_log\"))\n",
    "    conn.commit()\n",
    "    print(\"✅ Tabella data_quality_log pulita\")\n",
    "\n",
    "# Poi richiama la funzione corretta\n",
    "log_data_quality_metrics(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "604624bf-472b-4a2e-bb25-b4e0dc0074b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 FASE 4: VERIFICA FINALE\n",
      "------------------------------------------------------------\n",
      "📊 STATISTICHE FLUSSO DATI:\n",
      "Orders:    Source: 99,441 → RDL Valid: 99,441 → DW: 96,303\n",
      "Customers: Source: 99,441 → RDL Valid: 99,441 → DW: 99,441\n",
      "\n",
      "🏆 METRICHE QUALITÀ:\n",
      "Qualità media RDL: 100.0%\n",
      "Qualità minima: 100.0%\n",
      "Qualità massima: 100.0%\n",
      "Tabelle verificate: 8\n",
      "\n",
      "Dettaglio per tipo di controllo:\n",
      "  - accuracy: 100.0% (su 1 controlli)\n",
      "  - completeness: 100.0% (su 1 controlli)\n",
      "  - consistency: 100.0% (su 1 controlli)\n",
      "  - validation: 100.0% (su 8 controlli)\n",
      "\n",
      "Top 3 tabelle per qualità:\n",
      "  ✅ geolocation: 100.0%\n",
      "  ✅ sellers: 100.0%\n",
      "  ✅ order_items: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FASE 4: VERIFICA FINALE E STATISTICHE\n",
    "# =============================================================================\n",
    "print(\"\\n📈 FASE 4: VERIFICA FINALE\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Confronto qualità: Source → RDL → DW\n",
    "with engine.connect() as conn:\n",
    "    # Conteggi originali\n",
    "    source_orders = len(source_data['orders'])\n",
    "    source_customers = len(source_data['customers'])\n",
    "    \n",
    "    # Conteggi RDL\n",
    "    rdl_valid_orders = conn.execute(text(\n",
    "        \"SELECT COUNT(*) FROM rdl.orders WHERE rdl_is_valid = true\"\n",
    "    )).scalar()\n",
    "    rdl_valid_customers = conn.execute(text(\n",
    "        \"SELECT COUNT(*) FROM rdl.customers WHERE rdl_is_valid = true\"\n",
    "    )).scalar()\n",
    "    \n",
    "    # Conteggi DW\n",
    "    dw_orders = conn.execute(text(\"SELECT COUNT(DISTINCT order_id) FROM fact_sales\")).scalar()\n",
    "    dw_customers = conn.execute(text(\"SELECT COUNT(*) FROM dim_customer\")).scalar()\n",
    "\n",
    "print(\"📊 STATISTICHE FLUSSO DATI:\")\n",
    "print(f\"Orders:    Source: {source_orders:,} → RDL Valid: {rdl_valid_orders:,} → DW: {dw_orders:,}\")\n",
    "print(f\"Customers: Source: {source_customers:,} → RDL Valid: {rdl_valid_customers:,} → DW: {dw_customers:,}\")\n",
    "\n",
    "# Quality metrics\n",
    "print(\"\\n🏆 METRICHE QUALITÀ:\")\n",
    "with engine.connect() as conn:\n",
    "    # Prima verifica se ci sono dati\n",
    "    count_check = conn.execute(text(\n",
    "        \"SELECT COUNT(*) FROM rdl.data_quality_log\"\n",
    "    )).scalar()\n",
    "    \n",
    "    if count_check > 0:\n",
    "        # Media generale\n",
    "        quality_metrics = conn.execute(text(\"\"\"\n",
    "            SELECT \n",
    "                AVG(quality_score) as avg_quality,\n",
    "                MIN(quality_score) as min_quality,\n",
    "                MAX(quality_score) as max_quality,\n",
    "                COUNT(DISTINCT table_name) as tables_checked\n",
    "            FROM rdl.data_quality_log\n",
    "            WHERE check_timestamp >= CURRENT_DATE - INTERVAL '1 day'\n",
    "        \"\"\")).fetchone()\n",
    "        \n",
    "        if quality_metrics[0]:\n",
    "            print(f\"Qualità media RDL: {quality_metrics[0]:.1f}%\")\n",
    "            print(f\"Qualità minima: {quality_metrics[1]:.1f}%\")\n",
    "            print(f\"Qualità massima: {quality_metrics[2]:.1f}%\")\n",
    "            print(f\"Tabelle verificate: {quality_metrics[3]}\")\n",
    "            \n",
    "            # Dettaglio per tipo di check\n",
    "            print(\"\\nDettaglio per tipo di controllo:\")\n",
    "            detail_query = \"\"\"\n",
    "                SELECT \n",
    "                    check_type,\n",
    "                    COUNT(*) as num_checks,\n",
    "                    AVG(quality_score) as avg_score\n",
    "                FROM rdl.data_quality_log\n",
    "                WHERE check_timestamp >= CURRENT_DATE - INTERVAL '1 day'\n",
    "                GROUP BY check_type\n",
    "                ORDER BY check_type\n",
    "            \"\"\"\n",
    "            details = conn.execute(text(detail_query)).fetchall()\n",
    "            for check_type, num_checks, avg_score in details:\n",
    "                print(f\"  - {check_type}: {avg_score:.1f}% (su {num_checks} controlli)\")\n",
    "            \n",
    "            # Top 3 migliori e peggiori\n",
    "            print(\"\\nTop 3 tabelle per qualità:\")\n",
    "            top_query = \"\"\"\n",
    "                SELECT table_name, AVG(quality_score) as avg_score\n",
    "                FROM rdl.data_quality_log\n",
    "                GROUP BY table_name\n",
    "                ORDER BY avg_score DESC\n",
    "                LIMIT 3\n",
    "            \"\"\"\n",
    "            top_tables = conn.execute(text(top_query)).fetchall()\n",
    "            for table, score in top_tables:\n",
    "                print(f\"  ✅ {table}: {score:.1f}%\")\n",
    "                \n",
    "    else:\n",
    "        print(\"⚠️ Nessuna metrica di qualità registrata.\")\n",
    "        print(\"Le metriche verranno raccolte nei prossimi run del processo ETL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "672398ba-6b41-49cf-ae66-6c55b98cfeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔨 FASE 5: CREAZIONE VISTE MATERIALIZZATE\n",
      "------------------------------------------------------------\n",
      "✅ Vista audit trail creata\n",
      "✅ Viste DW refreshed\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FASE 5: CREAZIONE VISTE MATERIALIZZATE\n",
    "# =============================================================================\n",
    "print(\"\\n🔨 FASE 5: CREAZIONE VISTE MATERIALIZZATE\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Le tue viste esistenti più alcune nuove per RDL\n",
    "with engine.connect() as conn:\n",
    "    # Vista audit trail\n",
    "    conn.execute(text(\"\"\"\n",
    "        CREATE MATERIALIZED VIEW IF NOT EXISTS mv_etl_audit AS\n",
    "        SELECT \n",
    "            process_name,\n",
    "            process_type,\n",
    "            start_timestamp,\n",
    "            end_timestamp,\n",
    "            status,\n",
    "            records_processed,\n",
    "            records_rejected,\n",
    "            EXTRACT(EPOCH FROM (end_timestamp - start_timestamp)) as duration_seconds\n",
    "        FROM rdl.etl_process_log\n",
    "        ORDER BY start_timestamp DESC\n",
    "    \"\"\"))\n",
    "    \n",
    "    print(\"✅ Vista audit trail creata\")\n",
    "    \n",
    "    # Refresh viste esistenti\n",
    "    conn.execute(text(\"REFRESH MATERIALIZED VIEW mv_geographic_sales\"))\n",
    "    conn.execute(text(\"REFRESH MATERIALIZED VIEW mv_category_performance\"))\n",
    "    print(\"✅ Viste DW refreshed\")\n",
    "    \n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68b8780f-6310-4a69-92c2-57d5ae44e61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🎉 PIPELINE ETL 3-LAYER COMPLETATA CON SUCCESSO!\n",
      "============================================================\n",
      "\n",
      "📋 RIEPILOGO ARCHITETTURA:\n",
      "1️⃣ SOURCE LAYER: File CSV grezzi\n",
      "2️⃣ RDL LAYER: Dati validati, arricchiti, con quality scores\n",
      "3️⃣ DW LAYER: Star schema pronto per analisi OLAP\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONCLUSIONE\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 PIPELINE ETL 3-LAYER COMPLETATA CON SUCCESSO!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n📋 RIEPILOGO ARCHITETTURA:\")\n",
    "print(\"1️⃣ SOURCE LAYER: File CSV grezzi\")\n",
    "print(\"2️⃣ RDL LAYER: Dati validati, arricchiti, con quality scores\")\n",
    "print(\"3️⃣ DW LAYER: Star schema pronto per analisi OLAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9aa148d-f225-4fc4-ae1e-1536122f4be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 VERIFICA COMPLETAMENTO ETL - VERSIONE COMPLETA\n",
      "============================================================\n",
      "\n",
      "📊 SCHEMA RDL:\n",
      " table_name  record_count\n",
      "  customers         99441\n",
      "geolocation        720148\n",
      "order_items        112650\n",
      "     orders         99441\n",
      "   payments        103886\n",
      "   products         32951\n",
      "    reviews         98410\n",
      "    sellers          3095\n",
      "\n",
      "Totale record RDL: 1,270,022\n",
      "\n",
      "📊 SCHEMA DATA WAREHOUSE:\n",
      "   table_name  record_count\n",
      " dim_customer         99441\n",
      "dim_geography         19015\n",
      "  dim_payment            28\n",
      "  dim_product         32951\n",
      "   dim_seller          3095\n",
      "     dim_time           634\n",
      "   fact_sales        115358\n",
      "\n",
      "Totale record DW: 270,522\n",
      "\n",
      "📍 VERIFICA GEOGRAPHY:\n",
      "    description  total_records  unique_values\n",
      "RDL Geolocation         720148          19015\n",
      "   DW Geography          19015          19015\n",
      "\n",
      "🔗 TEST INTEGRITÀ:\n",
      "                   test  count  percentage\n",
      "Customers con geography  99163       99.72\n",
      "  Sellers con geography   3088       99.77\n",
      "\n",
      "✅ VERIFICA COMPLETATA!\n",
      "\n",
      "📊 VALORI ATTESI:\n",
      "   - rdl.geolocation: ~720,000 record\n",
      "   - dim_geography: ~19,000 record (ZIP unici)\n",
      "   - Customers/Sellers con geography_key: ~100%\n"
     ]
    }
   ],
   "source": [
    "print(\"🔍 VERIFICA COMPLETAMENTO ETL - VERSIONE COMPLETA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Verifica schema RDL (include geolocation)\n",
    "print(\"\\n📊 SCHEMA RDL:\")\n",
    "rdl_query = \"\"\"\n",
    "SELECT table_name, record_count\n",
    "FROM (\n",
    "    SELECT 'orders' as table_name, COUNT(*) as record_count FROM rdl.orders\n",
    "    UNION ALL SELECT 'customers', COUNT(*) FROM rdl.customers\n",
    "    UNION ALL SELECT 'products', COUNT(*) FROM rdl.products\n",
    "    UNION ALL SELECT 'order_items', COUNT(*) FROM rdl.order_items\n",
    "    UNION ALL SELECT 'payments', COUNT(*) FROM rdl.payments\n",
    "    UNION ALL SELECT 'reviews', COUNT(*) FROM rdl.reviews\n",
    "    UNION ALL SELECT 'sellers', COUNT(*) FROM rdl.sellers\n",
    "    UNION ALL SELECT 'geolocation', COUNT(*) FROM rdl.geolocation  -- AGGIUNTA\n",
    ") t\n",
    "ORDER BY table_name\n",
    "\"\"\"\n",
    "rdl_counts = pd.read_sql(rdl_query, engine)\n",
    "print(rdl_counts.to_string(index=False))\n",
    "print(f\"\\nTotale record RDL: {rdl_counts['record_count'].sum():,}\")\n",
    "\n",
    "# 2. Verifica schema DW (include dim_geography)\n",
    "print(\"\\n📊 SCHEMA DATA WAREHOUSE:\")\n",
    "dw_query = \"\"\"\n",
    "SELECT table_name, record_count\n",
    "FROM (\n",
    "    SELECT 'dim_geography' as table_name, COUNT(*) as record_count FROM dim_geography  -- AGGIUNTA\n",
    "    UNION ALL SELECT 'dim_customer', COUNT(*) FROM dim_customer\n",
    "    UNION ALL SELECT 'dim_product', COUNT(*) FROM dim_product\n",
    "    UNION ALL SELECT 'dim_seller', COUNT(*) FROM dim_seller\n",
    "    UNION ALL SELECT 'dim_time', COUNT(*) FROM dim_time\n",
    "    UNION ALL SELECT 'dim_payment', COUNT(*) FROM dim_payment\n",
    "    UNION ALL SELECT 'fact_sales', COUNT(*) FROM fact_sales\n",
    ") t\n",
    "ORDER BY table_name\n",
    "\"\"\"\n",
    "dw_counts = pd.read_sql(dw_query, engine)\n",
    "print(dw_counts.to_string(index=False))\n",
    "print(f\"\\nTotale record DW: {dw_counts['record_count'].sum():,}\")\n",
    "\n",
    "# 3. Verifica specifica geography/geolocation\n",
    "print(\"\\n📍 VERIFICA GEOGRAPHY:\")\n",
    "geo_check = \"\"\"\n",
    "SELECT \n",
    "    'RDL Geolocation' as description,\n",
    "    COUNT(*) as total_records,\n",
    "    COUNT(DISTINCT geolocation_zip_code_prefix) as unique_values\n",
    "FROM rdl.geolocation\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'DW Geography',\n",
    "    COUNT(*),\n",
    "    COUNT(DISTINCT zip_code_prefix)\n",
    "FROM dim_geography\n",
    "\"\"\"\n",
    "geo_result = pd.read_sql(geo_check, engine)\n",
    "print(geo_result.to_string(index=False))\n",
    "\n",
    "# 4. Test integrità\n",
    "print(\"\\n🔗 TEST INTEGRITÀ:\")\n",
    "integrity_check = \"\"\"\n",
    "SELECT \n",
    "    'Customers con geography' as test,\n",
    "    COUNT(*) as count,\n",
    "    ROUND(100.0 * COUNT(*) / (SELECT COUNT(*) FROM dim_customer), 2) as percentage\n",
    "FROM dim_customer\n",
    "WHERE geography_key IS NOT NULL\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Sellers con geography',\n",
    "    COUNT(*),\n",
    "    ROUND(100.0 * COUNT(*) / (SELECT COUNT(*) FROM dim_seller), 2)\n",
    "FROM dim_seller\n",
    "WHERE geography_key IS NOT NULL\n",
    "\"\"\"\n",
    "integrity = pd.read_sql(integrity_check, engine)\n",
    "print(integrity.to_string(index=False))\n",
    "\n",
    "# 5. Status finale\n",
    "print(\"\\n✅ VERIFICA COMPLETATA!\")\n",
    "print(\"\\n📊 VALORI ATTESI:\")\n",
    "print(\"   - rdl.geolocation: ~720,000 record\")\n",
    "print(\"   - dim_geography: ~19,000 record (ZIP unici)\")\n",
    "print(\"   - Customers/Sellers con geography_key: ~100%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
