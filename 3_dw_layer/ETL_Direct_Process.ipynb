{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97e871f1-93b5-4a6b-aec0-3f0946cc6a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELLA 1: IMPORTS\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "import psycopg2\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "950b41e7-299e-416f-b45e-91da74cb0afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connessione PostgreSQL riuscita!\n",
      "Versione: PostgreSQL 17.5 on x86_64-windows, compiled by msvc-19.43.34808, 64-bit\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELLA 2: TEST CONNESSIONE DATABASE\n",
    "# =============================================================================\n",
    "engine = create_engine('postgresql://postgres:postgres@localhost:5432/ecommerce')\n",
    "\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT version()\"))\n",
    "        version = result.fetchone()[0]\n",
    "        print(\"✅ Connessione PostgreSQL riuscita!\")\n",
    "        print(f\"Versione: {version}\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Errore connessione:\")\n",
    "    print(str(e))\n",
    "    print(\"\\n🔧 Verifica:\")\n",
    "    print(\"1. PostgreSQL è avviato?\")\n",
    "    print(\"2. Password corretta?\")\n",
    "    print(\"3. Database 'ecommerce' esiste?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31c7fcb7-663d-4bbc-b1f0-2e378e126719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Ricerca file CSV nella struttura 3-layer...\n",
      "📂 Directory corrente: C:\\Users\\Leonardo\\Desktop\\PROGETTO_DM\\3_dw_layer\n",
      "✅ Trovato percorso valido: ../1_source_layer/datasets_olist\n",
      "\n",
      "📊 Trovati 13 file CSV totali:\n",
      "   📄 ../1_source_layer/datasets_olist\\olist_customers_dataset.csv\n",
      "   📄 ../1_source_layer/datasets_olist\\olist_geolocation_dataset.csv\n",
      "   📄 ../1_source_layer/datasets_olist\\olist_orders_dataset.csv\n",
      "   📄 ../1_source_layer/datasets_olist\\olist_order_items_dataset.csv\n",
      "   📄 ../1_source_layer/datasets_olist\\olist_order_payments_dataset.csv\n",
      "   📄 ../1_source_layer/datasets_olist\\olist_order_reviews_dataset.csv\n",
      "   📄 ../1_source_layer/datasets_olist\\olist_products_dataset.csv\n",
      "   📄 ../1_source_layer/datasets_olist\\olist_sellers_dataset.csv\n",
      "   📄 ../1_source_layer/datasets_olist\\product_category_name_translation.csv\n",
      "   📄 ../1_source_layer/datasets_olist\\.ipynb_checkpoints\\olist_geolocation_dataset-checkpoint.csv\n",
      "   ... e altri 3 file\n",
      "\n",
      "🛒 File Olist trovati (13):\n",
      "   📄 olist_customers_dataset.csv\n",
      "   📄 olist_geolocation_dataset.csv\n",
      "   📄 olist_orders_dataset.csv\n",
      "   📄 olist_order_items_dataset.csv\n",
      "   📄 olist_order_payments_dataset.csv\n",
      "   📄 olist_order_reviews_dataset.csv\n",
      "   📄 olist_products_dataset.csv\n",
      "   📄 olist_sellers_dataset.csv\n",
      "   📄 product_category_name_translation.csv\n",
      "   📄 olist_geolocation_dataset-checkpoint.csv\n",
      "   📄 olist_orders_dataset-checkpoint.csv\n",
      "   📄 olist_order_payments_dataset-checkpoint.csv\n",
      "   📄 product_category_name_translation-checkpoint.csv\n",
      "\n",
      "📋 Verifica completezza dataset:\n",
      "   ✅ olist_orders_dataset.csv\n",
      "   ✅ olist_order_items_dataset.csv\n",
      "   ✅ olist_customers_dataset.csv\n",
      "   ✅ olist_products_dataset.csv\n",
      "   ✅ olist_sellers_dataset.csv\n",
      "   ✅ olist_order_payments_dataset.csv\n",
      "   ✅ olist_order_reviews_dataset.csv\n",
      "   ✅ olist_geolocation_dataset.csv\n",
      "   ✅ product_category_name_translation.csv\n",
      "\n",
      "🧪 Test caricamento: olist_customers_dataset.csv\n",
      "✅ Caricamento riuscito! Righe: 99,441, Colonne: 5\n",
      "Prime colonne: ['customer_id', 'customer_unique_id', 'customer_zip_code_prefix', 'customer_city', 'customer_state']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELLA 3: TROVA FILE CSV\n",
    "# =============================================================================\n",
    "import os\n",
    "print(\"🔍 Ricerca file CSV nella struttura 3-layer...\")\n",
    "\n",
    "# Determina la directory corrente e la struttura del progetto\n",
    "current_dir = os.getcwd()\n",
    "print(f\"📂 Directory corrente: {current_dir}\")\n",
    "\n",
    "def find_csv_files(directory):\n",
    "    \"\"\"Trova tutti i file CSV in una directory e sottodirectory\"\"\"\n",
    "    csv_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                full_path = os.path.join(root, file)\n",
    "                csv_files.append(full_path)\n",
    "    return csv_files\n",
    "\n",
    "# Possibili percorsi per i dati sorgente nella nuova struttura\n",
    "possible_paths = [\n",
    "    '1_source_layer/datasets_olist',          # Se eseguito dalla root del progetto\n",
    "    '../1_source_layer/datasets_olist',       # Se eseguito da una sottocartella\n",
    "    '../../1_source_layer/datasets_olist',     # Se eseguito da una sottocartella più profonda\n",
    "    'datasets_olist',                          # Struttura originale (fallback)\n",
    "    '../datasets_olist',                       # Struttura originale da sottocartella\n",
    "    '.'                                        # Directory corrente\n",
    "]\n",
    "\n",
    "# Trova il percorso corretto\n",
    "csv_files = []\n",
    "correct_path = None\n",
    "\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"✅ Trovato percorso valido: {path}\")\n",
    "        csv_files = find_csv_files(path)\n",
    "        if csv_files:\n",
    "            correct_path = path\n",
    "            break\n",
    "        else:\n",
    "            print(f\"   ⚠️ Percorso esiste ma nessun CSV trovato\")\n",
    "\n",
    "if not correct_path:\n",
    "    print(\"❌ Nessun percorso valido trovato. Cercando in tutta la struttura...\")\n",
    "    # Cerca dalla directory corrente\n",
    "    csv_files = find_csv_files('.')\n",
    "\n",
    "# Mostra i file trovati\n",
    "print(f\"\\n📊 Trovati {len(csv_files)} file CSV totali:\")\n",
    "if len(csv_files) > 0:\n",
    "    for file in csv_files[:10]:  # Mostra solo i primi 10\n",
    "        print(f\"   📄 {file}\")\n",
    "    if len(csv_files) > 10:\n",
    "        print(f\"   ... e altri {len(csv_files) - 10} file\")\n",
    "\n",
    "# Cerca specificamente i file Olist\n",
    "olist_files = [f for f in csv_files if 'olist' in f.lower()]\n",
    "print(f\"\\n🛒 File Olist trovati ({len(olist_files)}):\")\n",
    "for file in olist_files:\n",
    "    print(f\"   📄 {os.path.basename(file)}\")\n",
    "\n",
    "# Verifica struttura dataset Olist\n",
    "expected_files = [\n",
    "    'olist_orders_dataset.csv',\n",
    "    'olist_order_items_dataset.csv',\n",
    "    'olist_customers_dataset.csv',\n",
    "    'olist_products_dataset.csv',\n",
    "    'olist_sellers_dataset.csv',\n",
    "    'olist_order_payments_dataset.csv',\n",
    "    'olist_order_reviews_dataset.csv',\n",
    "    'olist_geolocation_dataset.csv',\n",
    "    'product_category_name_translation.csv'\n",
    "]\n",
    "\n",
    "print(\"\\n📋 Verifica completezza dataset:\")\n",
    "found_names = [os.path.basename(f) for f in olist_files]\n",
    "for expected in expected_files:\n",
    "    if expected in found_names:\n",
    "        print(f\"   ✅ {expected}\")\n",
    "    else:\n",
    "        print(f\"   ❌ {expected} - MANCANTE!\")\n",
    "\n",
    "# Prova a caricare un file per test\n",
    "if olist_files:\n",
    "    test_file = olist_files[0]\n",
    "    print(f\"\\n🧪 Test caricamento: {os.path.basename(test_file)}\")\n",
    "    try:\n",
    "        df_test = pd.read_csv(test_file)\n",
    "        print(f\"✅ Caricamento riuscito! Righe: {len(df_test):,}, Colonne: {len(df_test.columns)}\")\n",
    "        print(\"Prime colonne:\", list(df_test.columns[:5]))\n",
    "        \n",
    "        # Suggerisci il BASE_PATH corretto per le celle successive\n",
    "        if correct_path:\n",
    "            suggested_base = os.path.join(correct_path, '')  # Aggiunge / finale\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Errore caricamento: {e}\")\n",
    "else:\n",
    "    print(\"❌ Nessun file Olist trovato!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe9f9bef-eebc-4273-a40d-8dceaabf4d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Caricamento dataset...\n",
      "✅ Usando percorso rilevato: 1_source_layer/datasets_olist/\n",
      "\n",
      "📂 Caricamento da: ../1_source_layer/datasets_olist/\n",
      "------------------------------------------------------------\n",
      "📊 Caricando olist_orders_dataset.csv... ✅ 99,441 righe\n",
      "📊 Caricando olist_order_items_dataset.csv... ✅ 112,650 righe\n",
      "📊 Caricando olist_customers_dataset.csv... ✅ 99,441 righe\n",
      "📊 Caricando olist_products_dataset.csv... ✅ 32,951 righe\n",
      "📊 Caricando olist_sellers_dataset.csv... ✅ 3,095 righe\n",
      "📊 Caricando olist_order_payments_dataset.csv... ✅ 103,886 righe\n",
      "📊 Caricando olist_order_reviews_dataset.csv... ✅ 99,224 righe\n",
      "📊 Caricando olist_geolocation_dataset.csv... ✅ 1,000,163 righe\n",
      "📊 Caricando product_category_name_translation.csv... ✅ 71 righe\n",
      "------------------------------------------------------------\n",
      "\n",
      "✅ Tutti i dataset caricati con successo!\n",
      "\n",
      "📊 Riepilogo dataset:\n",
      "   Orders:      99,441 righe\n",
      "   Order Items: 112,650 righe\n",
      "   Customers:   99,441 righe\n",
      "   Products:    32,951 righe\n",
      "   Sellers:     3,095 righe\n",
      "   Payments:    103,886 righe\n",
      "   Reviews:     99,224 righe\n",
      "   Geolocation: 1,000,163 righe\n",
      "   Translation: 71 categorie\n",
      "\n",
      "📈 Statistiche rapide:\n",
      "   Periodo ordini: 2016-09-04 - 2018-10-17\n",
      "   Stati coperti: 27\n",
      "   Categorie prodotti: 73\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELLA 4: CARICAMENTO DATASET\n",
    "# =============================================================================\n",
    "print(\"📁 Caricamento dataset...\")\n",
    "BASE_PATH = '1_source_layer/datasets_olist/'\n",
    "print(f\"✅ Usando percorso rilevato: {BASE_PATH}\")\n",
    "    \n",
    "# Verifica se il percorso esiste\n",
    "if not os.path.exists(BASE_PATH):\n",
    "    # Prova percorsi alternativi\n",
    "    alt_paths = ['../1_source_layer/datasets_olist/', 'datasets_olist/', '../datasets_olist/']\n",
    "    for alt_path in alt_paths:\n",
    "        if os.path.exists(alt_path):\n",
    "            BASE_PATH = alt_path\n",
    "            break\n",
    "    else:\n",
    "        print(f\"❌ Percorso {BASE_PATH} non trovato!\")\n",
    "        print(\"🔧 Assicurati di:\")\n",
    "        print(\"   1. Essere nella directory root del progetto\")\n",
    "        print(\"   2. Aver creato la struttura 3-layer\")\n",
    "        print(\"   3. Aver copiato i CSV in 1_source_layer/datasets/olist/\")\n",
    "\n",
    "# Lista dei file da caricare\n",
    "files_to_load = {\n",
    "    'orders': 'olist_orders_dataset.csv',\n",
    "    'order_items': 'olist_order_items_dataset.csv', \n",
    "    'customers': 'olist_customers_dataset.csv',\n",
    "    'products': 'olist_products_dataset.csv',\n",
    "    'sellers': 'olist_sellers_dataset.csv',\n",
    "    'payments': 'olist_order_payments_dataset.csv',\n",
    "    'reviews': 'olist_order_reviews_dataset.csv',\n",
    "    'geolocation': 'olist_geolocation_dataset.csv',\n",
    "    'translation': 'product_category_name_translation.csv'\n",
    "}\n",
    "\n",
    "# Carica i dataset con gestione errori\n",
    "datasets = {}\n",
    "failed_files = []\n",
    "\n",
    "print(f\"\\n📂 Caricamento da: {BASE_PATH}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for dataset_name, filename in files_to_load.items():\n",
    "    filepath = os.path.join(BASE_PATH, filename)\n",
    "    try:\n",
    "        print(f\"📊 Caricando {filename}...\", end='')\n",
    "        datasets[dataset_name] = pd.read_csv(filepath)\n",
    "        print(f\" ✅ {len(datasets[dataset_name]):,} righe\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\" ❌ Non trovato\")\n",
    "        failed_files.append(filename)\n",
    "        # Prova percorso alternativo senza join\n",
    "        try:\n",
    "            alt_filepath = BASE_PATH + filename\n",
    "            datasets[dataset_name] = pd.read_csv(alt_filepath)\n",
    "            print(f\"   ✅ Trovato con percorso alternativo: {len(datasets[dataset_name]):,} righe\")\n",
    "            failed_files.remove(filename)\n",
    "        except:\n",
    "            pass\n",
    "    except Exception as e:\n",
    "        print(f\" ❌ Errore: {str(e)[:50]}...\")\n",
    "        failed_files.append(filename)\n",
    "\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Assegna alle variabili globali solo se caricamento riuscito\n",
    "if len(failed_files) == 0:\n",
    "    orders = datasets['orders']\n",
    "    order_items = datasets['order_items']\n",
    "    customers = datasets['customers']\n",
    "    products = datasets['products']\n",
    "    sellers = datasets['sellers']\n",
    "    payments = datasets['payments']\n",
    "    reviews = datasets['reviews']\n",
    "    geolocation = datasets['geolocation']\n",
    "    translation = datasets['translation']\n",
    "    \n",
    "    print(f\"\\n✅ Tutti i dataset caricati con successo!\")\n",
    "    print(\"\\n📊 Riepilogo dataset:\")\n",
    "    print(f\"   Orders:      {len(orders):,} righe\")\n",
    "    print(f\"   Order Items: {len(order_items):,} righe\")\n",
    "    print(f\"   Customers:   {len(customers):,} righe\")\n",
    "    print(f\"   Products:    {len(products):,} righe\")\n",
    "    print(f\"   Sellers:     {len(sellers):,} righe\")\n",
    "    print(f\"   Payments:    {len(payments):,} righe\")\n",
    "    print(f\"   Reviews:     {len(reviews):,} righe\")\n",
    "    print(f\"   Geolocation: {len(geolocation):,} righe\")\n",
    "    print(f\"   Translation: {len(translation):,} categorie\")\n",
    "    \n",
    "    # Mostra informazioni aggiuntive\n",
    "    print(f\"\\n📈 Statistiche rapide:\")\n",
    "    print(f\"   Periodo ordini: {orders['order_purchase_timestamp'].min()[:10]} - {orders['order_purchase_timestamp'].max()[:10]}\")\n",
    "    print(f\"   Stati coperti: {customers['customer_state'].nunique()}\")\n",
    "    print(f\"   Categorie prodotti: {products['product_category_name'].nunique()}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n❌ Caricamento fallito per {len(failed_files)} file:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80ab699f-ecf4-4d94-ace1-09adb9ef0c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Pulizia completa database...\n",
      "🗑️ Eliminazione FACT_SALES...\n",
      "🗑️ Eliminazione dimensioni...\n",
      "✅ Database pulito con successo!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELLA 5: PULIZIA COMPLETA DATABASE\n",
    "# =============================================================================\n",
    "def clean_database():\n",
    "    \"\"\"Pulisce tutte le tabelle nell'ordine corretto per rispettare i vincoli FK\"\"\"\n",
    "    print(\"🧹 Pulizia completa database...\")\n",
    "    \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            # 1. Prima elimina la fact table (che ha le FK)\n",
    "            print(\"🗑️ Eliminazione FACT_SALES...\")\n",
    "            conn.execute(text(\"DELETE FROM fact_sales\"))\n",
    "            \n",
    "            # 2. Poi elimina le dimensioni\n",
    "            print(\"🗑️ Eliminazione dimensioni...\")\n",
    "            conn.execute(text(\"DELETE FROM dim_customer\"))\n",
    "            conn.execute(text(\"DELETE FROM dim_product\"))\n",
    "            conn.execute(text(\"DELETE FROM dim_seller\"))\n",
    "            conn.execute(text(\"DELETE FROM dim_time\"))\n",
    "            conn.execute(text(\"DELETE FROM dim_payment\"))\n",
    "            conn.execute(text(\"DELETE FROM dim_geography\"))  # Nuova dimensione\n",
    "            \n",
    "            # 3. Commit tutte le operazioni\n",
    "            conn.commit()\n",
    "            print(\"✅ Database pulito con successo!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Errore pulizia database: {e}\")\n",
    "        raise\n",
    "\n",
    "# Esegui la pulizia\n",
    "clean_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd960939-b183-4ff4-bcfd-5c00a1cebf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗺️ PREPARAZIONE DATI GEOGRAFICI...\n",
      "📍 Creando lookup geografica...\n",
      "✅ Lookup geografica creata: 19,015 ZIP codes unici\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELLA 6: PREPARAZIONE GEOGRAFIA E CALCOLO DISTANZE\n",
    "# =============================================================================\n",
    "print(\"🗺️ PREPARAZIONE DATI GEOGRAFICI...\")\n",
    "\n",
    "# Crea lookup table geografica ottimizzata\n",
    "print(\"📍 Creando lookup geografica...\")\n",
    "geo_lookup = geolocation.groupby('geolocation_zip_code_prefix').agg({\n",
    "    'geolocation_lat': 'mean',  # Media delle coordinate per ZIP\n",
    "    'geolocation_lng': 'mean',\n",
    "    'geolocation_city': 'first',  # Prima città per ZIP\n",
    "    'geolocation_state': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"✅ Lookup geografica creata: {len(geo_lookup):,} ZIP codes unici\")\n",
    "\n",
    "# Funzione per calcolo distanze\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calcola distanza in km tra due punti usando formula haversine\"\"\"\n",
    "    if pd.isna(lat1) or pd.isna(lon1) or pd.isna(lat2) or pd.isna(lon2):\n",
    "        return None\n",
    "    \n",
    "    # Converti in radianti\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Formula haversine\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    r = 6371  # Raggio Terra in km\n",
    "    return c * r\n",
    "\n",
    "# Funzione per determinare regione\n",
    "def get_region(state):\n",
    "    regions = {\n",
    "        'AC': 'Norte', 'AP': 'Norte', 'AM': 'Norte', 'PA': 'Norte', \n",
    "        'RO': 'Norte', 'RR': 'Norte', 'TO': 'Norte',\n",
    "        'AL': 'Nordeste', 'BA': 'Nordeste', 'CE': 'Nordeste', 'MA': 'Nordeste',\n",
    "        'PB': 'Nordeste', 'PE': 'Nordeste', 'PI': 'Nordeste', 'RN': 'Nordeste', 'SE': 'Nordeste',\n",
    "        'GO': 'Centro-Oeste', 'MT': 'Centro-Oeste', 'MS': 'Centro-Oeste', 'DF': 'Centro-Oeste',\n",
    "        'ES': 'Sudeste', 'MG': 'Sudeste', 'RJ': 'Sudeste', 'SP': 'Sudeste',\n",
    "        'PR': 'Sul', 'RS': 'Sul', 'SC': 'Sul'\n",
    "    }\n",
    "    return regions.get(state, 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c20f298c-0efb-42b3-a634-411da981bc6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌍 ETL DIM_GEOGRAPHY...\n",
      "✅ DIM_GEOGRAPHY preparata: 19,015 ZIP codes\n",
      "✅ DIM_GEOGRAPHY caricata: 19,015 righe\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELLA 7: ETL DIM_GEOGRAPHY (NUOVA DIMENSIONE)\n",
    "# =============================================================================\n",
    "print(\"🌍 ETL DIM_GEOGRAPHY...\")\n",
    "\n",
    "# Prepara dimensione geografia\n",
    "dim_geography_data = geo_lookup.copy()\n",
    "dim_geography_data = dim_geography_data.rename(columns={\n",
    "    'geolocation_zip_code_prefix': 'zip_code_prefix',\n",
    "    'geolocation_lat': 'latitude',\n",
    "    'geolocation_lng': 'longitude',\n",
    "    'geolocation_city': 'city',\n",
    "    'geolocation_state': 'state'\n",
    "})\n",
    "\n",
    "# Assicura che zip_code_prefix sia string\n",
    "dim_geography_data['zip_code_prefix'] = dim_geography_data['zip_code_prefix'].astype(str)\n",
    "\n",
    "# Aggiungi regione\n",
    "dim_geography_data['region'] = dim_geography_data['state'].apply(get_region)\n",
    "\n",
    "print(f\"✅ DIM_GEOGRAPHY preparata: {len(dim_geography_data):,} ZIP codes\")\n",
    "\n",
    "# Carica nel database\n",
    "try:\n",
    "    dim_geography_data.to_sql('dim_geography', engine, if_exists='append', index=False, method='multi')\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT COUNT(*) FROM dim_geography\"))\n",
    "        count = result.fetchone()[0]\n",
    "        print(f\"✅ DIM_GEOGRAPHY caricata: {count:,} righe\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Errore caricamento DIM_GEOGRAPHY: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b2e908c-d6f8-4eeb-98bd-b20decfe8ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 ETL DIM_CUSTOMER (con coordinate)...\n",
      "✅ DIM_CUSTOMER preparata: 99,441 righe\n",
      "   Con coordinate: 99,163 clienti\n",
      "✅ DIM_CUSTOMER caricata: 99,441 righe\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELLA 8: ETL DIM_CUSTOMER CON COORDINATE\n",
    "# =============================================================================\n",
    "print(\"🔄 ETL DIM_CUSTOMER (con coordinate)...\")\n",
    "\n",
    "# Merge customers con geolocation\n",
    "customers_enriched = customers.merge(\n",
    "    geo_lookup, \n",
    "    left_on='customer_zip_code_prefix', \n",
    "    right_on='geolocation_zip_code_prefix', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Pulisci e prepara\n",
    "customers_clean = customers_enriched.drop_duplicates(subset=['customer_id'])\n",
    "customers_clean['customer_city'] = customers_clean['customer_city'].fillna('Unknown')\n",
    "customers_clean['customer_state'] = customers_clean['customer_state'].fillna('XX')\n",
    "\n",
    "# Ottieni geography_key\n",
    "dim_geography_keys = pd.read_sql(\"SELECT geography_key, zip_code_prefix FROM dim_geography\", engine)\n",
    "\n",
    "# Prepara dati finali\n",
    "dim_customer_data = customers_clean[[\n",
    "    'customer_id', 'customer_city', 'customer_state', \n",
    "    'customer_zip_code_prefix', 'geolocation_lat', 'geolocation_lng'\n",
    "]].copy()\n",
    "\n",
    "dim_customer_data = dim_customer_data.rename(columns={\n",
    "    'geolocation_lat': 'customer_latitude',\n",
    "    'geolocation_lng': 'customer_longitude'\n",
    "})\n",
    "\n",
    "# Converti zip_code_prefix a string in entrambi i DataFrame per evitare type mismatch\n",
    "dim_customer_data['customer_zip_code_prefix'] = dim_customer_data['customer_zip_code_prefix'].astype(str)\n",
    "dim_geography_keys['zip_code_prefix'] = dim_geography_keys['zip_code_prefix'].astype(str)\n",
    "\n",
    "# Aggiungi geography_key\n",
    "dim_customer_data = dim_customer_data.merge(\n",
    "    dim_geography_keys,\n",
    "    left_on='customer_zip_code_prefix',\n",
    "    right_on='zip_code_prefix',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Rimuovi colonna duplicata\n",
    "dim_customer_data = dim_customer_data.drop('zip_code_prefix', axis=1)\n",
    "\n",
    "print(f\"✅ DIM_CUSTOMER preparata: {len(dim_customer_data):,} righe\")\n",
    "\n",
    "# Carica nel database\n",
    "try:\n",
    "    dim_customer_data.to_sql('dim_customer', engine, if_exists='append', index=False, method='multi', chunksize=1000)\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT COUNT(*) FROM dim_customer\"))\n",
    "        count = result.fetchone()[0]\n",
    "        print(f\"✅ DIM_CUSTOMER caricata: {count:,} righe\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Errore caricamento DIM_CUSTOMER: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2e05d32-5d3c-4f3b-9004-8fd93c0496b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 ETL DIM_PRODUCT (con traduzioni e classificazioni)...\n",
      "🔗 Aggiungendo traduzioni categorie...\n",
      "✅ DIM_PRODUCT preparata: 32,951 righe\n",
      "Top categorie (inglese):\n",
      "product_category_name_english\n",
      "bed_bath_table     3029\n",
      "sports_leisure     2867\n",
      "furniture_decor    2657\n",
      "health_beauty      2444\n",
      "housewares         2335\n",
      "Name: count, dtype: int64\n",
      "✅ DIM_PRODUCT caricata: 32,951 righe\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELLA 9: ETL DIM_PRODUCT CON TRADUZIONI E CLASSIFICAZIONI\n",
    "# =============================================================================\n",
    "print(\"🔄 ETL DIM_PRODUCT (con traduzioni e classificazioni)...\")\n",
    "\n",
    "# Pulizia products\n",
    "products_clean = products.copy()\n",
    "products_clean = products_clean.drop_duplicates(subset=['product_id'])\n",
    "products_clean['product_category_name'] = products_clean['product_category_name'].fillna('outros')\n",
    "\n",
    "# Join con traduzioni\n",
    "print(\"🔗 Aggiungendo traduzioni categorie...\")\n",
    "products_with_translation = products_clean.merge(\n",
    "    translation[['product_category_name', 'product_category_name_english']], \n",
    "    on='product_category_name', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Se non c'è traduzione, usa il nome originale\n",
    "products_with_translation['product_category_name_english'] = (\n",
    "    products_with_translation['product_category_name_english']\n",
    "    .fillna(products_with_translation['product_category_name'])\n",
    ")\n",
    "\n",
    "# Funzioni di classificazione\n",
    "def classify_weight(weight):\n",
    "    if pd.isna(weight) or weight == 0:\n",
    "        return 'Unknown'\n",
    "    elif weight < 500:\n",
    "        return 'Light'\n",
    "    elif weight < 2000:\n",
    "        return 'Medium'\n",
    "    elif weight < 5000:\n",
    "        return 'Heavy'\n",
    "    else:\n",
    "        return 'Very Heavy'\n",
    "\n",
    "def classify_size(length, height, width):\n",
    "    if pd.isna(length) or pd.isna(height) or pd.isna(width):\n",
    "        return 'Unknown'\n",
    "    volume = length * height * width\n",
    "    if volume < 1000:\n",
    "        return 'Small'\n",
    "    elif volume < 8000:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Large'\n",
    "\n",
    "# Applica classificazioni\n",
    "products_with_translation['weight_category'] = products_with_translation['product_weight_g'].apply(classify_weight)\n",
    "products_with_translation['size_category'] = products_with_translation.apply(\n",
    "    lambda x: classify_size(x['product_length_cm'], x['product_height_cm'], x['product_width_cm']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Riempi valori numerici mancanti\n",
    "dim_product_data = products_with_translation.fillna(0)\n",
    "\n",
    "print(f\"✅ DIM_PRODUCT preparata: {len(dim_product_data):,} righe\")\n",
    "print(\"Top categorie (inglese):\")\n",
    "print(dim_product_data['product_category_name_english'].value_counts().head())\n",
    "\n",
    "# Carica nel database\n",
    "try:\n",
    "    dim_product_data.to_sql('dim_product', engine, if_exists='append', index=False, method='multi', chunksize=1000)\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT COUNT(*) FROM dim_product\"))\n",
    "        count = result.fetchone()[0]\n",
    "        print(f\"✅ DIM_PRODUCT caricata: {count:,} righe\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Errore caricamento DIM_PRODUCT: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50460405-7d14-4ba1-b779-bdb8c9df582c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 ETL DIM_SELLER (con coordinate)...\n",
      "✅ DIM_SELLER preparata: 3,095 righe\n",
      "   Con coordinate: 3,088 venditori\n",
      "✅ DIM_SELLER caricata: 3,095 righe\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELLA 10: ETL DIM_SELLER CON COORDINATE\n",
    "# =============================================================================\n",
    "print(\"🔄 ETL DIM_SELLER (con coordinate)...\")\n",
    "\n",
    "# Merge sellers con geolocation\n",
    "sellers_enriched = sellers.merge(\n",
    "    geo_lookup, \n",
    "    left_on='seller_zip_code_prefix', \n",
    "    right_on='geolocation_zip_code_prefix', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Pulisci e prepara\n",
    "sellers_clean = sellers_enriched.drop_duplicates(subset=['seller_id'])\n",
    "sellers_clean['seller_city'] = sellers_clean['seller_city'].fillna('Unknown')\n",
    "sellers_clean['seller_state'] = sellers_clean['seller_state'].fillna('XX')\n",
    "\n",
    "# Prepara dati finali\n",
    "dim_seller_data = sellers_clean[[\n",
    "    'seller_id', 'seller_city', 'seller_state', \n",
    "    'seller_zip_code_prefix', 'geolocation_lat', 'geolocation_lng'\n",
    "]].copy()\n",
    "\n",
    "dim_seller_data = dim_seller_data.rename(columns={\n",
    "    'geolocation_lat': 'seller_latitude',\n",
    "    'geolocation_lng': 'seller_longitude'\n",
    "})\n",
    "\n",
    "# Converti zip_code_prefix a string per evitare type mismatch\n",
    "dim_seller_data['seller_zip_code_prefix'] = dim_seller_data['seller_zip_code_prefix'].astype(str)\n",
    "# dim_geography_keys['zip_code_prefix'] è già convertito nella cella 8\n",
    "\n",
    "# Aggiungi geography_key\n",
    "dim_seller_data = dim_seller_data.merge(\n",
    "    dim_geography_keys,\n",
    "    left_on='seller_zip_code_prefix',\n",
    "    right_on='zip_code_prefix',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Rimuovi colonna duplicata\n",
    "dim_seller_data = dim_seller_data.drop('zip_code_prefix', axis=1)\n",
    "\n",
    "print(f\"✅ DIM_SELLER preparata: {len(dim_seller_data):,} righe\")\n",
    "\n",
    "# Carica nel database\n",
    "try:\n",
    "    dim_seller_data.to_sql('dim_seller', engine, if_exists='append', index=False, method='multi', chunksize=1000)\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT COUNT(*) FROM dim_seller\"))\n",
    "        count = result.fetchone()[0]\n",
    "        print(f\"✅ DIM_SELLER caricata: {count:,} righe\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Errore caricamento DIM_SELLER: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5e43d4d-08b7-466c-9507-fcc72c5d7400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 ETL DIM_TIME (con stagioni)...\n",
      "✅ DIM_TIME preparata: 634 righe\n",
      "Range date: 2016-09-04 - 2018-10-17\n",
      "✅ DIM_TIME caricata: 634 righe\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELLA 11: ETL DIM_TIME CON STAGIONI BRASILIANE\n",
    "# =============================================================================\n",
    "print(\"🔄 ETL DIM_TIME (con stagioni)...\")\n",
    "\n",
    "# Estrai tutte le date uniche dagli ordini\n",
    "orders['order_purchase_timestamp'] = pd.to_datetime(orders['order_purchase_timestamp'])\n",
    "unique_dates = orders['order_purchase_timestamp'].dt.date.unique()\n",
    "\n",
    "# Funzione per stagione brasiliana\n",
    "def get_season_brazil(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Verão'  # Estate\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Outono'  # Autunno\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Inverno'  # Inverno\n",
    "    else:\n",
    "        return 'Primavera'  # Primavera\n",
    "\n",
    "# Crea dimensione tempo\n",
    "time_data = []\n",
    "for date in unique_dates:\n",
    "    if pd.notna(date):\n",
    "        dt = pd.to_datetime(date)\n",
    "        time_data.append({\n",
    "            'full_date': date,\n",
    "            'day_of_week': dt.dayofweek,\n",
    "            'day_name': dt.strftime('%A'),\n",
    "            'day_of_month': dt.day,\n",
    "            'month_num': dt.month,\n",
    "            'month_name': dt.strftime('%B'),\n",
    "            'quarter': dt.quarter,\n",
    "            'year': dt.year,\n",
    "            'is_weekend': dt.dayofweek >= 5,\n",
    "            'season_brazil': get_season_brazil(dt.month)\n",
    "        })\n",
    "\n",
    "dim_time_data = pd.DataFrame(time_data)\n",
    "dim_time_data = dim_time_data.sort_values('full_date')\n",
    "\n",
    "print(f\"✅ DIM_TIME preparata: {len(dim_time_data):,} righe\")\n",
    "print(f\"Range date: {dim_time_data['full_date'].min()} - {dim_time_data['full_date'].max()}\")\n",
    "\n",
    "# Carica nel database\n",
    "try:\n",
    "    dim_time_data.to_sql('dim_time', engine, if_exists='append', index=False, method='multi')\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT COUNT(*) FROM dim_time\"))\n",
    "        count = result.fetchone()[0]\n",
    "        print(f\"✅ DIM_TIME caricata: {count:,} righe\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Errore caricamento DIM_TIME: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5351bb3-f106-4537-80fb-45f07f5efff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 ETL DIM_PAYMENT (con categorie)...\n",
      "✅ DIM_PAYMENT preparata: 28 righe\n",
      "Tipi di pagamento:\n",
      "payment_type\n",
      "credit_card    24\n",
      "boleto          1\n",
      "voucher         1\n",
      "debit_card      1\n",
      "not_defined     1\n",
      "Name: count, dtype: int64\n",
      "✅ DIM_PAYMENT caricata: 28 righe\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELLA 12: ETL DIM_PAYMENT CON CATEGORIE\n",
    "# =============================================================================\n",
    "print(\"🔄 ETL DIM_PAYMENT (con categorie)...\")\n",
    "\n",
    "# Funzione per classificare rate\n",
    "def classify_installments(installments):\n",
    "    if installments == 1:\n",
    "        return 'Cash/Single Payment'\n",
    "    elif installments <= 6:\n",
    "        return 'Short Term'\n",
    "    elif installments <= 12:\n",
    "        return 'Medium Term'\n",
    "    else:\n",
    "        return 'Long Term'\n",
    "\n",
    "# Crea combinazioni uniche payment_type + installments\n",
    "payment_combinations = payments[['payment_type', 'payment_installments']].drop_duplicates()\n",
    "payment_combinations = payment_combinations.fillna({'payment_installments': 1})\n",
    "\n",
    "# Aggiungi categorizzazione\n",
    "payment_combinations['installment_category'] = payment_combinations['payment_installments'].apply(classify_installments)\n",
    "\n",
    "dim_payment_data = payment_combinations.copy()\n",
    "\n",
    "print(f\"✅ DIM_PAYMENT preparata: {len(dim_payment_data):,} righe\")\n",
    "print(\"Tipi di pagamento:\")\n",
    "print(dim_payment_data['payment_type'].value_counts())\n",
    "\n",
    "# Carica nel database\n",
    "try:\n",
    "    dim_payment_data.to_sql('dim_payment', engine, if_exists='append', index=False, method='multi')\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT COUNT(*) FROM dim_payment\"))\n",
    "        count = result.fetchone()[0]\n",
    "        print(f\"✅ DIM_PAYMENT caricata: {count:,} righe\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Errore caricamento DIM_PAYMENT: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b88f8037-c166-43c7-9ba2-2ee71c26d5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 ETL FACT_SALES (preparazione)...\n",
      "📊 Joining datasets...\n",
      "✅ Fact base preparata: 110,197 righe\n",
      "Periodo: 2016-09-15 - 2018-08-29\n",
      "Valore totale: €13,221,498.11\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELLA 13: ETL FACT TABLE - PREPARAZIONE\n",
    "# =============================================================================\n",
    "print(\"🔄 ETL FACT_SALES (preparazione)...\")\n",
    "\n",
    "# Unisci tutti i dataset per creare il fatto\n",
    "print(\"📊 Joining datasets...\")\n",
    "\n",
    "# Base: order_items\n",
    "fact_base = order_items.copy()\n",
    "\n",
    "# Join con orders per data\n",
    "fact_base = fact_base.merge(\n",
    "    orders[['order_id', 'customer_id', 'order_purchase_timestamp', 'order_status']], \n",
    "    on='order_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Solo ordini delivered\n",
    "fact_base = fact_base[fact_base['order_status'] == 'delivered']\n",
    "\n",
    "# Join con payments (aggrega per ordine)\n",
    "payments_agg = payments.groupby('order_id').agg({\n",
    "    'payment_type': 'first',  # Prendi il primo metodo di pagamento\n",
    "    'payment_installments': 'first',\n",
    "    'payment_value': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "fact_base = fact_base.merge(payments_agg, on='order_id', how='left')\n",
    "\n",
    "# Join con reviews (aggrega per ordine)\n",
    "reviews_agg = reviews.groupby('order_id').agg({\n",
    "    'review_score': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "fact_base = fact_base.merge(reviews_agg, on='order_id', how='left')\n",
    "\n",
    "# Pulisci e prepara\n",
    "fact_base['order_purchase_timestamp'] = pd.to_datetime(fact_base['order_purchase_timestamp'])\n",
    "fact_base['order_date'] = fact_base['order_purchase_timestamp'].dt.date\n",
    "\n",
    "# Calcola misure derivate\n",
    "fact_base['total_item_value'] = fact_base['price'] + fact_base['freight_value']\n",
    "fact_base['quantity'] = 1  # Ogni riga è un item\n",
    "\n",
    "# Rimuovi righe con valori critici mancanti\n",
    "fact_base = fact_base.dropna(subset=['customer_id', 'product_id', 'seller_id', 'order_date'])\n",
    "\n",
    "print(f\"✅ Fact base preparata: {len(fact_base):,} righe\")\n",
    "print(f\"Periodo: {fact_base['order_date'].min()} - {fact_base['order_date'].max()}\")\n",
    "print(f\"Valore totale: €{fact_base['price'].sum():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4418bd2a-f08c-4b86-b82e-da3db9bb309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Creazione foreign keys e calcolo metriche geografiche...\n",
      "📏 Calcolando distanze customer-seller...\n",
      "✅ Foreign keys create e metriche calcolate!\n",
      "Righe prima pulizia: 110,197\n",
      "Righe dopo pulizia: 110,197\n",
      "Righe rimosse: 0\n",
      "📊 Statistiche distanze:\n",
      "   - Media: 596.2 km\n",
      "   - Vendite cross-state: 70,331\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELLA 14: CREAZIONE FOREIGN KEYS E CALCOLO METRICHE\n",
    "# =============================================================================\n",
    "print(\"🔗 Creazione foreign keys e calcolo metriche geografiche...\")\n",
    "\n",
    "# Carica le dimensioni con le chiavi surrogate E coordinate\n",
    "dim_customer_keys = pd.read_sql(\"\"\"\n",
    "    SELECT customer_key, customer_id, customer_latitude, customer_longitude, customer_state \n",
    "    FROM dim_customer\n",
    "\"\"\", engine)\n",
    "\n",
    "dim_product_keys = pd.read_sql(\"SELECT product_key, product_id FROM dim_product\", engine)\n",
    "\n",
    "dim_seller_keys = pd.read_sql(\"\"\"\n",
    "    SELECT seller_key, seller_id, seller_latitude, seller_longitude, seller_state \n",
    "    FROM dim_seller\n",
    "\"\"\", engine)\n",
    "\n",
    "dim_time_keys = pd.read_sql(\"SELECT time_key, full_date FROM dim_time\", engine)\n",
    "\n",
    "dim_payment_keys = pd.read_sql(\"\"\"\n",
    "    SELECT payment_key, payment_type, payment_installments \n",
    "    FROM dim_payment\n",
    "\"\"\", engine)\n",
    "\n",
    "# Join per ottenere le foreign keys E coordinate\n",
    "fact_with_keys = fact_base.copy()\n",
    "\n",
    "# Customer key e coordinate\n",
    "fact_with_keys = fact_with_keys.merge(dim_customer_keys, on='customer_id', how='left')\n",
    "\n",
    "# Product key\n",
    "fact_with_keys = fact_with_keys.merge(dim_product_keys, on='product_id', how='left')\n",
    "\n",
    "# Seller key e coordinate\n",
    "fact_with_keys = fact_with_keys.merge(dim_seller_keys, on='seller_id', how='left')\n",
    "\n",
    "# Time key\n",
    "fact_with_keys = fact_with_keys.merge(dim_time_keys, left_on='order_date', right_on='full_date', how='left')\n",
    "\n",
    "# Payment key\n",
    "fact_with_keys = fact_with_keys.merge(dim_payment_keys, on=['payment_type', 'payment_installments'], how='left')\n",
    "\n",
    "# Calcola distanze e metriche geografiche\n",
    "print(\"📏 Calcolando distanze customer-seller...\")\n",
    "fact_with_keys['customer_seller_distance_km'] = fact_with_keys.apply(\n",
    "    lambda row: haversine_distance(\n",
    "        row['customer_latitude'], row['customer_longitude'],\n",
    "        row['seller_latitude'], row['seller_longitude']\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Flag cross-state\n",
    "fact_with_keys['is_cross_state_sale'] = (\n",
    "    fact_with_keys['customer_state'] != fact_with_keys['seller_state']\n",
    ")\n",
    "\n",
    "# Shipping type\n",
    "fact_with_keys['shipping_type'] = fact_with_keys.apply(\n",
    "    lambda x: 'Local' if x['customer_state'] == x['seller_state'] else 'Interstate',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Metriche derivate\n",
    "fact_with_keys['freight_percentage'] = (\n",
    "    fact_with_keys['freight_value'] / fact_with_keys['price'] * 100\n",
    ").round(2)\n",
    "fact_with_keys['net_revenue'] = fact_with_keys['price'] - fact_with_keys['freight_value']\n",
    "\n",
    "# Seleziona colonne finali per fact table\n",
    "fact_final = fact_with_keys[[\n",
    "    'customer_key', 'product_key', 'time_key', 'seller_key', 'payment_key',\n",
    "    'order_id', 'order_item_id',\n",
    "    'price', 'freight_value', 'quantity', 'payment_value', 'review_score',\n",
    "    'total_item_value', 'customer_seller_distance_km', 'is_cross_state_sale', \n",
    "    'shipping_type', 'freight_percentage', 'net_revenue'\n",
    "]].copy()\n",
    "\n",
    "# Rimuovi righe con foreign keys mancanti\n",
    "before_count = len(fact_final)\n",
    "fact_final = fact_final.dropna(subset=['customer_key', 'product_key', 'time_key', 'seller_key'])\n",
    "after_count = len(fact_final)\n",
    "\n",
    "print(f\"✅ Foreign keys create e metriche calcolate!\")\n",
    "print(f\"Righe prima pulizia: {before_count:,}\")\n",
    "print(f\"Righe dopo pulizia: {after_count:,}\")\n",
    "print(f\"Righe rimosse: {before_count - after_count:,}\")\n",
    "print(f\"📊 Statistiche distanze:\")\n",
    "print(f\"   - Media: {fact_final['customer_seller_distance_km'].mean():.1f} km\")\n",
    "print(f\"   - Vendite cross-state: {fact_final['is_cross_state_sale'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc5d05e4-b8c0-4a0f-8b6d-de4e12029d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Caricamento FACT_SALES...\n",
      "Caricato batch 1: 5000 righe\n",
      "Caricato batch 2: 5000 righe\n",
      "Caricato batch 3: 5000 righe\n",
      "Caricato batch 4: 5000 righe\n",
      "Caricato batch 5: 5000 righe\n",
      "Caricato batch 6: 5000 righe\n",
      "Caricato batch 7: 5000 righe\n",
      "Caricato batch 8: 5000 righe\n",
      "Caricato batch 9: 5000 righe\n",
      "Caricato batch 10: 5000 righe\n",
      "Caricato batch 11: 5000 righe\n",
      "Caricato batch 12: 5000 righe\n",
      "Caricato batch 13: 5000 righe\n",
      "Caricato batch 14: 5000 righe\n",
      "Caricato batch 15: 5000 righe\n",
      "Caricato batch 16: 5000 righe\n",
      "Caricato batch 17: 5000 righe\n",
      "Caricato batch 18: 5000 righe\n",
      "Caricato batch 19: 5000 righe\n",
      "Caricato batch 20: 5000 righe\n",
      "Caricato batch 21: 5000 righe\n",
      "Caricato batch 22: 5000 righe\n",
      "Caricato batch 23: 197 righe\n",
      "✅ FACT_SALES caricata: 110,197 righe\n",
      "📈 Statistiche finali:\n",
      "   Prezzo min: €0.85\n",
      "   Prezzo max: €6735.00\n",
      "   Prezzo medio: €119.98\n",
      "   Revenue totale: €13,221,498.11\n",
      "   Distanza media: 596.2 km\n",
      "   Vendite cross-state: 70,331\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELLA 15: CARICA FACT_SALES ARRICCHITA\n",
    "# =============================================================================\n",
    "print(\"📊 Caricamento FACT_SALES...\")\n",
    "\n",
    "try:\n",
    "    # Carica in batch per performance\n",
    "    batch_size = 5000\n",
    "    total_rows = len(fact_final)\n",
    "    \n",
    "    for i in range(0, total_rows, batch_size):\n",
    "        batch = fact_final.iloc[i:i+batch_size]\n",
    "        batch.to_sql('fact_sales', engine, if_exists='append', index=False, method='multi')\n",
    "        print(f\"Caricato batch {i//batch_size + 1}: {len(batch)} righe\")\n",
    "    \n",
    "    # Verifica caricamento finale\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT COUNT(*) FROM fact_sales\"))\n",
    "        count = result.fetchone()[0]\n",
    "        print(f\"✅ FACT_SALES caricata: {count:,} righe\")\n",
    "        \n",
    "        # Statistiche finali\n",
    "        stats = conn.execute(text(\"\"\"\n",
    "            SELECT \n",
    "                MIN(price) as min_price,\n",
    "                MAX(price) as max_price,\n",
    "                AVG(price) as avg_price,\n",
    "                SUM(price) as total_revenue,\n",
    "                AVG(customer_seller_distance_km) as avg_distance,\n",
    "                SUM(CASE WHEN is_cross_state_sale THEN 1 ELSE 0 END) as cross_state_sales\n",
    "            FROM fact_sales\n",
    "            WHERE price IS NOT NULL\n",
    "        \"\"\"))\n",
    "        row = stats.fetchone()\n",
    "        \n",
    "        if row and row[0] is not None:\n",
    "            print(f\"📈 Statistiche finali:\")\n",
    "            print(f\"   Prezzo min: €{row[0]:.2f}\")\n",
    "            print(f\"   Prezzo max: €{row[1]:.2f}\")\n",
    "            print(f\"   Prezzo medio: €{row[2]:.2f}\")\n",
    "            print(f\"   Revenue totale: €{row[3]:,.2f}\")\n",
    "            print(f\"   Distanza media: {row[4]:.1f} km\")\n",
    "            print(f\"   Vendite cross-state: {row[5]:,}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Errore caricamento FACT_SALES: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccdd0147-15e2-42d1-9de1-4334fe48d030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔨 Creazione viste materializzate per performance...\n",
      "📍 Creando vista materializzata geografica...\n",
      "📦 Creando vista materializzata categorie...\n",
      "📅 Creando vista materializzata temporale...\n",
      "✅ Viste materializzate create con successo!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELLA 16: CREAZIONE VISTE MATERIALIZZATE\n",
    "# =============================================================================\n",
    "print(\"\\n🔨 Creazione viste materializzate per performance...\")\n",
    "\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        # Vista geografica\n",
    "        print(\"📍 Creando vista materializzata geografica...\")\n",
    "        conn.execute(text(\"DROP MATERIALIZED VIEW IF EXISTS mv_geographic_sales\"))\n",
    "        conn.execute(text(\"\"\"\n",
    "            CREATE MATERIALIZED VIEW mv_geographic_sales AS\n",
    "            SELECT \n",
    "                dc.customer_state,\n",
    "                ds.seller_state,\n",
    "                dg_c.region as customer_region,\n",
    "                dg_s.region as seller_region,\n",
    "                COUNT(*) as total_sales,\n",
    "                AVG(fs.customer_seller_distance_km) as avg_distance,\n",
    "                SUM(CASE WHEN fs.is_cross_state_sale THEN 1 ELSE 0 END) as cross_state_sales,\n",
    "                SUM(fs.price) as total_revenue,\n",
    "                AVG(fs.freight_value) as avg_freight,\n",
    "                AVG(fs.freight_percentage) as avg_freight_percentage\n",
    "            FROM fact_sales fs\n",
    "            JOIN dim_customer dc ON fs.customer_key = dc.customer_key\n",
    "            JOIN dim_seller ds ON fs.seller_key = ds.seller_key\n",
    "            LEFT JOIN dim_geography dg_c ON dc.geography_key = dg_c.geography_key\n",
    "            LEFT JOIN dim_geography dg_s ON ds.geography_key = dg_s.geography_key\n",
    "            GROUP BY dc.customer_state, ds.seller_state, dg_c.region, dg_s.region\n",
    "        \"\"\"))\n",
    "        \n",
    "        # Vista categorie\n",
    "        print(\"📦 Creando vista materializzata categorie...\")\n",
    "        conn.execute(text(\"DROP MATERIALIZED VIEW IF EXISTS mv_category_performance\"))\n",
    "        conn.execute(text(\"\"\"\n",
    "            CREATE MATERIALIZED VIEW mv_category_performance AS\n",
    "            SELECT \n",
    "                dp.product_category_name,\n",
    "                dp.product_category_name_english,\n",
    "                dp.weight_category,\n",
    "                dp.size_category,\n",
    "                COUNT(*) as total_sales,\n",
    "                SUM(fs.price) as total_revenue,\n",
    "                AVG(fs.review_score) as avg_rating,\n",
    "                AVG(fs.customer_seller_distance_km) as avg_shipping_distance,\n",
    "                AVG(fs.freight_percentage) as avg_freight_percentage\n",
    "            FROM fact_sales fs\n",
    "            JOIN dim_product dp ON fs.product_key = dp.product_key\n",
    "            WHERE dp.product_category_name_english IS NOT NULL\n",
    "            GROUP BY dp.product_category_name, dp.product_category_name_english, \n",
    "                     dp.weight_category, dp.size_category\n",
    "        \"\"\"))\n",
    "        \n",
    "        # Vista performance temporale\n",
    "        print(\"📅 Creando vista materializzata temporale...\")\n",
    "        conn.execute(text(\"DROP MATERIALIZED VIEW IF EXISTS mv_temporal_performance\"))\n",
    "        conn.execute(text(\"\"\"\n",
    "            CREATE MATERIALIZED VIEW mv_temporal_performance AS\n",
    "            SELECT \n",
    "                dt.year,\n",
    "                dt.quarter,\n",
    "                dt.month_name,\n",
    "                dt.season_brazil,\n",
    "                dt.is_weekend,\n",
    "                COUNT(*) as total_sales,\n",
    "                SUM(fs.price) as total_revenue,\n",
    "                AVG(fs.price) as avg_order_value,\n",
    "                COUNT(DISTINCT fs.customer_key) as unique_customers,\n",
    "                AVG(fs.review_score) as avg_satisfaction\n",
    "            FROM fact_sales fs\n",
    "            JOIN dim_time dt ON fs.time_key = dt.time_key\n",
    "            GROUP BY dt.year, dt.quarter, dt.month_name, dt.month_num, \n",
    "                     dt.season_brazil, dt.is_weekend\n",
    "            ORDER BY dt.year, dt.month_num\n",
    "        \"\"\"))\n",
    "        \n",
    "        conn.commit()\n",
    "        print(\"✅ Viste materializzate create con successo!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Errore creazione viste: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35df7e87-d73a-4ae7-b8ff-b8b43c7d0608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🎯 VERIFICA FINALE ETL COMPLETO\n",
      "============================================================\n",
      "📊 CONTEGGIO RIGHE:\n",
      "   DIM_GEOGRAPHY: 19,015 righe\n",
      "   DIM_CUSTOMER: 99,441 righe\n",
      "   DIM_PRODUCT: 32,951 righe\n",
      "   DIM_SELLER: 3,095 righe\n",
      "   DIM_TIME: 634 righe\n",
      "   DIM_PAYMENT: 28 righe\n",
      "   FACT_SALES: 110,197 righe\n",
      "\n",
      "✨ VERIFICHE ENRICHMENT:\n",
      "   📍 Regioni geografiche: 5\n",
      "   📍 ZIP codes totali: 19,015\n",
      "   👥 Customers con coordinate: 99,163 su 99,441 (99.7%)\n",
      "   📦 Categorie con traduzione: 64 su 64\n",
      "   📏 Distanza media vendite: 596.2 km\n",
      "   📏 Range distanze: 0.0 - 8677.9 km\n",
      "   🚛 Vendite cross-state: 63.7%\n",
      "\n",
      "🧪 TEST QUERY COMPLESSA:\n",
      "   ✅ Query multi-join con enrichment funzionante!\n",
      "\n",
      "🎉 ETL COMPLETATO CON SUCCESSO!\n",
      "\n",
      "📊 RIEPILOGO SCHEMA ENRICHED:\n",
      "   ✅ 6 dimensioni\n",
      "   ✅ Traduzioni categorie prodotti integrate\n",
      "   ✅ Coordinate GPS per customers e sellers\n",
      "   ✅ Distanze calcolate nella fact table\n",
      "   ✅ Classificazioni prodotti (peso/dimensione)\n",
      "   ✅ Categorizzazione pagamenti\n",
      "   ✅ Stagionalità brasiliana\n",
      "   ✅ 3 viste materializzate per performance\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELLA 17: VERIFICA FINALE\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎯 VERIFICA FINALE ETL COMPLETO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        # Conta righe in tutte le tabelle\n",
    "        tables = ['dim_geography', 'dim_customer', 'dim_product', 'dim_seller', \n",
    "                  'dim_time', 'dim_payment', 'fact_sales']\n",
    "        \n",
    "        print(\"📊 CONTEGGIO RIGHE:\")\n",
    "        for table in tables:\n",
    "            result = conn.execute(text(f\"SELECT COUNT(*) FROM {table}\"))\n",
    "            count = result.fetchone()[0]\n",
    "            print(f\"   {table.upper()}: {count:,} righe\")\n",
    "        \n",
    "        # Verifiche enrichment\n",
    "        print(\"\\n✨ VERIFICHE ENRICHMENT:\")\n",
    "        \n",
    "        # Geografia\n",
    "        result = conn.execute(text(\"\"\"\n",
    "            SELECT COUNT(DISTINCT region) as regions, COUNT(*) as total_zips\n",
    "            FROM dim_geography\n",
    "        \"\"\"))\n",
    "        geo_stats = result.fetchone()\n",
    "        print(f\"   📍 Regioni geografiche: {geo_stats[0]}\")\n",
    "        print(f\"   📍 ZIP codes totali: {geo_stats[1]:,}\")\n",
    "        \n",
    "        # Customers con coordinate\n",
    "        result = conn.execute(text(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total,\n",
    "                SUM(CASE WHEN customer_latitude IS NOT NULL THEN 1 ELSE 0 END) as with_coords\n",
    "            FROM dim_customer\n",
    "        \"\"\"))\n",
    "        cust_stats = result.fetchone()\n",
    "        print(f\"   👥 Customers con coordinate: {cust_stats[1]:,} su {cust_stats[0]:,} ({cust_stats[1]/cust_stats[0]*100:.1f}%)\")\n",
    "        \n",
    "        # Prodotti con traduzioni\n",
    "        result = conn.execute(text(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(DISTINCT product_category_name) as original,\n",
    "                COUNT(DISTINCT product_category_name_english) as translated\n",
    "            FROM dim_product\n",
    "            WHERE product_category_name_english != product_category_name\n",
    "        \"\"\"))\n",
    "        prod_stats = result.fetchone()\n",
    "        print(f\"   📦 Categorie con traduzione: {prod_stats[1]} su {prod_stats[0]}\")\n",
    "        \n",
    "        # Fact con distanze\n",
    "        result = conn.execute(text(\"\"\"\n",
    "            SELECT \n",
    "                AVG(customer_seller_distance_km) as avg_dist,\n",
    "                MIN(customer_seller_distance_km) as min_dist,\n",
    "                MAX(customer_seller_distance_km) as max_dist,\n",
    "                SUM(CASE WHEN is_cross_state_sale THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as cross_state_pct\n",
    "            FROM fact_sales\n",
    "            WHERE customer_seller_distance_km IS NOT NULL\n",
    "        \"\"\"))\n",
    "        fact_stats = result.fetchone()\n",
    "        print(f\"   📏 Distanza media vendite: {fact_stats[0]:.1f} km\")\n",
    "        print(f\"   📏 Range distanze: {fact_stats[1]:.1f} - {fact_stats[2]:.1f} km\")\n",
    "        print(f\"   🚛 Vendite cross-state: {fact_stats[3]:.1f}%\")\n",
    "        \n",
    "        # Test query complessa\n",
    "        print(\"\\n🧪 TEST QUERY COMPLESSA:\")\n",
    "        test_query = \"\"\"\n",
    "        SELECT \n",
    "            dp.product_category_name_english as category,\n",
    "            dg.region,\n",
    "            COUNT(*) as sales\n",
    "        FROM fact_sales fs\n",
    "        JOIN dim_product dp ON fs.product_key = dp.product_key\n",
    "        JOIN dim_customer dc ON fs.customer_key = dc.customer_key\n",
    "        JOIN dim_geography dg ON dc.geography_key = dg.geography_key\n",
    "        WHERE dp.product_category_name_english IS NOT NULL\n",
    "            AND dg.region != 'Unknown'\n",
    "        GROUP BY dp.product_category_name_english, dg.region\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "        result = conn.execute(text(test_query))\n",
    "        print(\"   ✅ Query multi-join con enrichment funzionante!\")\n",
    "        \n",
    "        print(\"\\n🎉 ETL COMPLETATO CON SUCCESSO!\")\n",
    "        print(\"\\n📊 RIEPILOGO SCHEMA ENRICHED:\")\n",
    "        print(\"   ✅ 6 dimensioni\")\n",
    "        print(\"   ✅ Traduzioni categorie prodotti integrate\")\n",
    "        print(\"   ✅ Coordinate GPS per customers e sellers\")\n",
    "        print(\"   ✅ Distanze calcolate nella fact table\")\n",
    "        print(\"   ✅ Classificazioni prodotti (peso/dimensione)\")\n",
    "        print(\"   ✅ Categorizzazione pagamenti\")\n",
    "        print(\"   ✅ Stagionalità brasiliana\")\n",
    "        print(\"   ✅ 3 viste materializzate per performance\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Errore verifica finale: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
